{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9e80fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Load Excel file\n",
    "df = pd.read_excel(\"318NewsDataSet.xlsx\")\n",
    "\n",
    "# Clean column names\n",
    "df.columns = [col.strip() for col in df.columns]\n",
    "corruptnDF = df.copy()\n",
    "\n",
    "# Check if 'Processed_Content' exists\n",
    "if 'Processed_Content' not in corruptnDF.columns:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess(text):\n",
    "        text = re.sub(r\"\\W+\", \" \", str(text).lower())\n",
    "        tokens = text.split()\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    corruptnDF['Processed_Content'] = corruptnDF['Content'].apply(preprocess)\n",
    "\n",
    "print(\"✅ 'Processed_Content' column is now ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80e8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel(\"318NewsDataSet.xlsx\")\n",
    "\n",
    "# Basic structure fix: assign df to corruptnDF\n",
    "corruptnDF = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'\\W+', ' ', str(text).lower())\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel(\"318NewsDataSet.xlsx\")\n",
    "\n",
    "# Check and handle missing 'Content' column\n",
    "if 'Content' not in df.columns:\n",
    "    raise KeyError(\"❌ ERROR: The column 'Content' is missing in the Excel file. Please ensure the file has a 'Content' column.\")\n",
    "\n",
    "# Ensure the 'Processed_Content' column exists (basic preprocessing fallback)\n",
    "if 'Processed_Content' not in df.columns:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    import re\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess(text):\n",
    "        text = re.sub(r\"\\W+\", \" \", str(text).lower())\n",
    "        tokens = text.split()\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    df['Processed_Content'] = df['Content'].apply(preprocess)\n",
    "\n",
    "print(\"✅ Data loaded and processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Excel data\n",
    "df = pd.read_excel(\"318NewsDataSet.xlsx\")\n",
    "\n",
    "# Check and handle missing 'Content' column\n",
    "if 'Content' not in df.columns:\n",
    "    raise KeyError(\"❌ ERROR: The column 'Content' is missing in the Excel file. Please ensure the file has a 'Content' column.\")\n",
    "\n",
    "# Ensure the 'Processed_Content' column exists (basic preprocessing fallback)\n",
    "if 'Processed_Content' not in df.columns:\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer\n",
    "    import re\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess(text):\n",
    "        text = re.sub(r\"\\W+\", \" \", str(text).lower())\n",
    "        tokens = text.split()\n",
    "        tokens = [lemmatizer.lemmatize(w) for w in tokens if w not in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    df['Processed_Content'] = df['Content'].apply(preprocess)\n",
    "\n",
    "print(\"✅ Data loaded and processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**A Multi-Dimensional Computational Data Analysis on News Data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **By Magdalena Davis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Setting up Environment and Data Acquisition**\n",
    "Library Installation and Updates. Ensuring all necessary libraries are installed and updated, for enabling data analysis. Imports relevant libraries for data manipulation, visualization, and preprocessing tools. Automates Dataset Download and Extraction, downloads and extracts the needed dataset without manual intervention.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing and updating necessary libraries for data downloading and analysis\n",
    "\n",
    "# Importing libraries for data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Ensuring the dataset is downloaded and extracted only if not already present\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Unzip only if the file is not yet extracted\n",
    "if not os.path.exists(\"318NewsDataSet.xlsx\"):\n",
    "    with zipfile.ZipFile(\"318NewsDataSet.xlsx.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall()  # Will extract the Excel file here\n",
    "\n",
    "# Now load the unzipped Excel file\n",
    "df = pd.read_excel(\"318NewsDataSet.xlsx\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# ⚠️ Commented out: nltk.download('punkt')\n",
    "# ⚠️ Commented out: nltk.download('stopwords')\n",
    "# ⚠️ Commented out: nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Create 'Processed_Content' column if not exists\n",
    "if 'Processed_Content' not in df.columns:\n",
    "    df['Processed_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# ⚠️ Commented out: nltk.download('punkt')\n",
    "# ⚠️ Commented out: nltk.download('stopwords')\n",
    "# ⚠️ Commented out: nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(str(text).lower())\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens if w.isalpha() and w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Create 'Processed_Content' column if not exists\n",
    "if 'Processed_Content' not in df.columns:\n",
    "    df['Processed_Content'] = df['Content'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Loads Data and Sets Up Visualization for Text Analysis**\n",
    "Library Importation for Display, Making DataFrame output easier to inspect and understand.\n",
    "Data loading, Reading Excel data into a structured pandas DataFrame for manipulation and analysis.\n",
    "Resource Download, Providing all NLTK text processing resources for NLP tasks. Visualisation Setup, Creating consistent and attractive visualization settings for subsequent plots.\n",
    "Lastly DataFrame Display for Quickly summarizing the dataset's structure and contents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Importing additional display tools for better visualization\n",
    "from IPython.display import display\n",
    "\n",
    "# Loading the data from an Excel file and defining DataFrame structure\n",
    "corpus_file = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(corpus_file, header=None,\n",
    "                           names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Downloading NLTK resources necessary for text processing\n",
    "# ⚠️ Commented out: nltk.download('popular', quiet=True)\n",
    "\n",
    "# Setting up visualization styles for consistent graphing\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Displaying the DataFrame in a more visually appealing format\n",
    "print(\"DataFrame Structure: \", corruptnDF.shape)\n",
    "print(\"First Few Rows Display:\")\n",
    "display(corruptnDF.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Identifies and Displays Duplicate Entries in the dataset**\n",
    "Duplicate Check Quickly detects duplicate rows to ensure data integrity. Count and Display Shows duplicates and allows further inspection.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code A: Searching for Duplicate Entries\n",
    "# Checking for duplicate entries in the dataset\n",
    "duplicate_entries = corruptnDF[corruptnDF.duplicated()]\n",
    "print(\"Duplicate Entries Found: \", duplicate_entries.shape[0])\n",
    "if not duplicate_entries.empty:\n",
    "    display(duplicate_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Handling of Missing Values**\n",
    "Dropping Missing Values Keeps only complete 'Content' column entries for analysis. Filling Missing Values, Removes NaN values from columns to avoid data processing issues.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling missing values\n",
    "corruptnDF = corruptnDF.dropna(subset=['Content'])  # Dropping rows where 'Content' is missing\n",
    "corruptnDF = corruptnDF.fillna('')  # Filling remaining missing values with empty strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Text Preprocessing for NLP Analysis**\n",
    "\n",
    "Configures the NLP environment, preprocesses text, and displays processed text. The text is cleaned and prepared for analysis using the spaCy library. Library importation, installs data handling and visualization tools. Model Download and Loading Prepares the NLP model without cluttering output. Preprocesses text data by removing unnecessary elements and simplifying words. Data loading and preprocessing Simplifies data loading and cleaning. Data Display Visually checks data before and after preprocessing for accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Data Integrity Validation and Consistency Checks**\n",
    "Data Integrity Check Finds missing columns to ensure the dataset is complete and ready for analysis. Print Statement immediately shows data completeness.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating data integrity\n",
    "print(\"Checking for missing values after processing:\")\n",
    "print(corruptnDF.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**DataFrame Overview and Document Length Distribution Analysis**\n",
    "Uses Info Display to shows data types and missing values in the DataFrame. Dataset's central tendency, dispersion, and shape are summarized in descriptive statistics. Each text document's length is calculated to analyze distribution. A Histogram Plot Shows document length distribution to identify patterns and outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display DataFrame information and descriptive statistics\n",
    "display(corruptnDF.info())  # Getting info on DataFrame to understand data types and non-null counts\n",
    "display(corruptnDF.describe(include='all'))  # Getting descriptive statistics for all columns, including categorical data\n",
    "\n",
    "# Plotting the distribution of document lengths\n",
    "doc_lengths = corruptnDF['Processed_Content'].dropna().apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(doc_lengths, kde=True, color='blue')\n",
    "plt.title('Distribution of Document Lengths')\n",
    "plt.xlabel('Document Length (number of words)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Interactive Visualisation of Document Length Distribution**\n",
    "Library import Installs interactive visualization tools. Interactive Plot Function Filters data and creates a histogram using user-defined criteria. Slider Widget provides Easy-to-use interface for dynamic document length setting. Interactive Widget Display enables Live plot updates to improve user engagement and data exploration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Implement interactive visualization using IPython widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def update_plot(max_len):\n",
    "    filtered_lengths = doc_lengths[doc_lengths <= max_len]\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(filtered_lengths, kde=True, color='blue')\n",
    "    plt.title('Filtered Distribution of Document Lengths')\n",
    "    plt.xlabel('Document Length (number of words)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "max_length_widget = widgets.IntSlider(\n",
    "    value=100,\n",
    "    min=0,\n",
    "    max=doc_lengths.max(),\n",
    "    step=1,\n",
    "    description='Max Length:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "widgets.interactive(update_plot, max_len=max_length_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Exploratory Data Analysis (EDA), Text Preprocessing and Sentiment Analysis**\n",
    "Preparing text data for analysis through cleaning and standardisation. Text sentiment Analysis Evaluating emotion. Data Integrity Check Verifying dataset completeness and analysis readiness. Visualization to Shows data distribution and relationships through plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Download required NLTK data quietly\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('punkt_tab', quiet=True) # Download the missing resource\n",
    "\n",
    "# Load the dataset\n",
    "corruptnDF = pd.read_excel(\"318NewsDataSet.xlsx\", header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# ... (Rest of your code remains the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data quietly\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset\n",
    "corruptnDF = pd.read_excel(\"318NewsDataSet.xlsx\", header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Content' column\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess_text)\n",
    "\n",
    "# Displaying the DataFrame in a more visually appealing format\n",
    "print(\"DataFrame Structure: \", corruptnDF.shape)\n",
    "print(\"First Few Rows Display:\")\n",
    "display(corruptnDF.head())\n",
    "\n",
    "# Enhanced sentiment analysis function\n",
    "def calculate_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Applying the sentiment analysis function to the 'Content'\n",
    "corruptnDF[['Sentiment_Polarity', 'Sentiment_Subjectivity']] = corruptnDF['Content'].apply(\n",
    "    lambda x: pd.Series(calculate_sentiment(x))\n",
    ")\n",
    "\n",
    "# Display the first few entries to check sentiment scores\n",
    "display(corruptnDF[['Content', 'Sentiment_Polarity', 'Sentiment_Subjectivity']].head())\n",
    "\n",
    "# Display the first few entries of the original and processed content\n",
    "display(corruptnDF[['Content', 'Processed_Content']].head())\n",
    "\n",
    "# Validating data integrity\n",
    "print(\"Checking for missing values after processing:\")\n",
    "print(corruptnDF.isnull().sum())\n",
    "\n",
    "# Visualizing the sentiment polarity distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(corruptnDF['Sentiment_Polarity'], kde=True, color='darkblue', alpha=0.8, label='Polarity')\n",
    "plt.title('Sentiment Polarity Distribution', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xlabel('Polarity', fontsize=12, fontweight='bold', color='black')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualizing the sentiment subjectivity distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(corruptnDF['Sentiment_Subjectivity'], kde=True, color='darkgreen', alpha=0.8, label='Subjectivity')\n",
    "plt.title('Sentiment Subjectivity Distribution', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xlabel('Subjectivity', fontsize=12, fontweight='bold', color='black')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of document lengths\n",
    "doc_lengths = corruptnDF['Processed_Content'].dropna().apply(lambda x: len(x.split()))\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(doc_lengths, kde=True, color='darkblue', alpha=0.8)\n",
    "plt.title('Distribution of Document Lengths', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xlabel('Document Length (number of words)', fontsize=12, fontweight='bold', color='black')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.show()\n",
    "\n",
    "# Additional EDA: Correlation matrix\n",
    "numerical_columns = corruptnDF.select_dtypes(include=['number']).columns\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corruptnDF[numerical_columns].corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.show()\n",
    "\n",
    "# Additional EDA: Box plot of document lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(y=doc_lengths, color='darkcyan', saturation=0.8)\n",
    "plt.title('Box Plot of Document Lengths', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xlabel('Document Length (number of words)', fontsize=12, fontweight='bold', color='black')\n",
    "plt.ylabel('Processed_Content', fontsize=12, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.show()\n",
    "\n",
    "# Additional EDA: Scatter plot of Sentiment Polarity vs. Subjectivity\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Sentiment_Polarity', y='Sentiment_Subjectivity', data=corruptnDF, alpha=0.6, color='darkblue')\n",
    "plt.title('Scatter Plot of Sentiment Polarity vs. Subjectivity', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xlabel('Sentiment Polarity', fontsize=12, fontweight='bold', color='black')\n",
    "plt.ylabel('Sentiment Subjectivity', fontsize=12, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Capping Outliers in Document Lengths**\n",
    "Text Preprocessing, Cleaning and standardizing text data for analysis. Limits extreme values to reduce outliers' impact on analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data quietly\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset\n",
    "corruptnDF = pd.read_excel(\"318NewsDataSet.xlsx\", header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    words = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Content' column\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess_text)\n",
    "\n",
    "# Calculate document lengths\n",
    "doc_lengths = corruptnDF['Processed_Content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Define the capping threshold (e.g., 95th percentile)\n",
    "cap_threshold = doc_lengths.quantile(0.95)\n",
    "\n",
    "# Cap the outliers\n",
    "doc_lengths_capped = doc_lengths.apply(lambda x: min(x, cap_threshold))\n",
    "\n",
    "# Store the new dataset with capped lengths in a separate DataFrame\n",
    "corruptnDF_capped = corruptnDF.copy()\n",
    "corruptnDF_capped['Capped_Document_Length'] = doc_lengths_capped\n",
    "\n",
    "# Save the new DataFrame as a separate dataset\n",
    "corruptnDF_capped.to_excel(\"318NewsDataSet_Capped.xlsx\", index=False)\n",
    "\n",
    "# Calculate the mean of the capped document lengths\n",
    "mean_length = doc_lengths_capped.mean()\n",
    "\n",
    "# Visualize the distribution of capped document lengths with enhanced colors, text, and a mean line\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(corruptnDF_capped['Capped_Document_Length'], kde=True, color='darkblue', alpha=0.8)\n",
    "plt.axvline(mean_length, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_length:.2f}')\n",
    "plt.title('Distribution of Document Lengths (Capped)', fontsize=14, fontweight='bold', color='black')\n",
    "plt.xlabel('Document Length (number of words)', fontsize=12, fontweight='bold', color='black')\n",
    "plt.ylabel('Frequency', fontsize=12, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=10, fontweight='bold', color='black')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Ensure original document lengths are preserved for other analyses\n",
    "corruptnDF['Document_Length'] = doc_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Visualising the DataFrame and Checking for Unique Values**\n",
    "Examining dataset's basic structure, summary statistics, and unique values to gain preliminary understanding and identify any presence of NAs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                           names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(corruptnDF.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(corruptnDF.describe(include='all'))\n",
    "\n",
    "# Display information about the dataframe\n",
    "print(corruptnDF.info())\n",
    "\n",
    "# Check for unique values in 'Year' and 'Month'\n",
    "print(corruptnDF['Year'].unique())\n",
    "print(corruptnDF['Month'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Data Cleaning of NAs and Visualisation of Article Distribution**\n",
    "Cleans the dataset, handles missing values, converts date fields to numeric, and visualizes article distribution by year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                           names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Calculate the total number of rows and NAs before cleaning\n",
    "total_rows = corruptnDF.shape[0]\n",
    "total_nas = corruptnDF['Year'].isna().sum() + corruptnDF['Month'].isna().sum()\n",
    "total_nas += (corruptnDF['Year'] == 'n.a').sum() + (corruptnDF['Year'] == 'n.a.').sum()\n",
    "total_nas += (corruptnDF['Month'] == 'n.a').sum() + (corruptnDF['Month'] == 'n.a.').sum()\n",
    "\n",
    "# Replace invalid month and year values with NaN\n",
    "corruptnDF['Year'].replace(['n.a', 'n.a.'], pd.NA, inplace=True)\n",
    "corruptnDF['Month'].replace(['n.a', 'n.a.'], pd.NA, inplace=True)\n",
    "\n",
    "# Strip leading/trailing spaces from month names\n",
    "corruptnDF['Month'] = corruptnDF['Month'].str.strip()\n",
    "\n",
    "# Drop rows with NaN in 'Year' or 'Month'\n",
    "corruptnDF.dropna(subset=['Year', 'Month'], inplace=True)\n",
    "\n",
    "# Convert 'Year' to numeric and drop remaining NaNs\n",
    "corruptnDF['Year'] = pd.to_numeric(corruptnDF['Year'], errors='coerce')\n",
    "corruptnDF.dropna(subset=['Year'], inplace=True)\n",
    "corruptnDF['Year'] = corruptnDF['Year'].astype(int)\n",
    "\n",
    "# Convert month names to numeric values\n",
    "month_dict = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "              'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12}\n",
    "corruptnDF['Month'] = corruptnDF['Month'].map(month_dict)\n",
    "\n",
    "# Drop rows with invalid months\n",
    "corruptnDF.dropna(subset=['Month'], inplace=True)\n",
    "corruptnDF['Month'] = corruptnDF['Month'].astype(int)\n",
    "\n",
    "# Calculate the number of articles per year and per month\n",
    "year_counts = corruptnDF['Year'].value_counts().sort_index()\n",
    "month_counts = corruptnDF['Month'].value_counts().sort_index()\n",
    "\n",
    "# Calculate averages\n",
    "average_year = year_counts.mean()\n",
    "average_month = month_counts.mean()\n",
    "\n",
    "# Total number of articles after cleaning\n",
    "total_articles = corruptnDF.shape[0]\n",
    "\n",
    "# Create visualizations\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Distribution of articles by year\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=year_counts.index, y=year_counts.values, palette='viridis')\n",
    "plt.axhline(average_year, color='red', linestyle='--', label=f'Average: {average_year:.2f} articles')\n",
    "plt.title('Distribution of Articles by Year', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Year', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Articles', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "for index, value in enumerate(year_counts.values):\n",
    "    plt.text(index, value + 0.5, f'{value}', ha='center', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "# Distribution of articles by month\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=month_counts.index, y=month_counts.values, palette='viridis')\n",
    "plt.axhline(average_month, color='red', linestyle='--', label=f'Average: {average_month:.2f} articles')\n",
    "plt.title('Distribution of Articles by Month', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Month', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Number of Articles', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45)\n",
    "for index, value in enumerate(month_counts.values):\n",
    "    plt.text(index, value + 0.5, f'{value}', ha='center', fontweight='bold')\n",
    "plt.legend()\n",
    "\n",
    "# Add a legend for the total number of articles and NAs\n",
    "plt.figtext(0.5, 0.01, f'Total Number of Articles: {total_articles}\\nTotal Number of NAs: {total_nas}', ha='center', fontsize=12, fontweight='bold', bbox={\"facecolor\": \"orange\", \"alpha\": 0.5, \"pad\": 5})\n",
    "\n",
    "plt.tight_layout(pad=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Text Cleaning and Preprocessing for Keyword Extraction**\n",
    "Text cleaning in natural language processing (NLP) removes noise and standardizes raw text into a format suitable for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Downloading NLTK resources without verbose output\n",
    "# ⚠️ Commented out: nltk.download('popular', quiet=True)\n",
    "\n",
    "# Initializing stop words, punctuation set, and lemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Defining the data cleaning function\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Load the dataset\n",
    "Corpus = './318NewsDataSet.xlsx'\n",
    "CorruptnDF = pd.read_excel(Corpus, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Defining corpus as the Content column\n",
    "corpus = CorruptnDF['Content'].tolist()\n",
    "\n",
    "# Text cleaning process\n",
    "CleanedKeyWords = [clean(doc).split() for doc in corpus]\n",
    "\n",
    "# To view the cleaned keywords\n",
    "print(CleanedKeyWords[:5])  # Printing the first 5 cleaned keywords for inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Text Cleaning and Keyword Extraction for NLP Analysis**\n",
    "Preprocesses and cleans text from the dataset by removing stop words, punctuation, and performs lemmatization for NLP tasks like keyword extraction and topic modeling. Text cleaning is an essential preprocessing stage in NLP that converts raw text into a standardized and noise-free format by eliminating irrelevant components and normalizing the text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Suppressing the output of library downloads\n",
    "# ⚠️ Commented out: nltk.download('popular', quiet=True)\n",
    "\n",
    "# Initialize stop words, punctuation set, and lemmatizer\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['said'])  # Add additional meaningless words to the stop words list\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# Define a function to clean and preprocess text\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Load the dataset\n",
    "corpus_path = \"318NewsDataSet.xlsx\"\n",
    "corruptnDF = pd.read_excel(corpus_path, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Define corpus as the Content column and clean it\n",
    "corpus = corruptnDF['Content'].tolist()\n",
    "\n",
    "# Print initial sample to verify content\n",
    "print(\"Initial sample of content:\", corpus[:5])\n",
    "\n",
    "cleaned_keywords = [clean(doc).split() for doc in corpus]\n",
    "\n",
    "# Print a sample of the cleaned keywords to ensure additional meaningless words are removed\n",
    "print(\"Sample of cleaned keywords:\", cleaned_keywords[:5])\n",
    "\n",
    "# Create a dictionary from the cleaned keywords\n",
    "keywords_dictionary = corpora.Dictionary(cleaned_keywords)\n",
    "\n",
    "# Display a sample of the dictionary content\n",
    "sample_dictionary_items = {k: keywords_dictionary[k] for k in list(keywords_dictionary)[:5]}\n",
    "print(\"Sample of dictionary items:\", sample_dictionary_items)\n",
    "\n",
    "# Optionally print all unique words (tokens) from the dictionary for further inspection\n",
    "# Uncomment the following lines to see the outputs\n",
    "# for key, value in sample_dictionary_items.items():\n",
    "#     print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Coherence Score Evaluation for Optimal LDA Topic Modeling**\n",
    "Calculates coherence scores for a range of topic numbers in Latent Dirichlet Allocation (LDA) models to determine the optimal topic number, improving topic model quality and interpretability. Coherence score calculation measures the semantic similarity of high-scoring words in LDA-generated topics to assess interpretability and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Assume cleaned and tokenized text data is available in CleanedKeyWords\n",
    "KeyWordsDictionary = corpora.Dictionary(CleanedKeyWords)\n",
    "doc_term_matrix = [KeyWordsDictionary.doc2bow(text) for text in CleanedKeyWords]\n",
    "\n",
    "# Function to calculate coherence score\n",
    "def calculate_coherence_score(num_topics):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=KeyWordsDictionary, passes=10, random_state=0)\n",
    "    coherence_model = CoherenceModel(model=lda_model, texts=CleanedKeyWords, dictionary=KeyWordsDictionary, coherence='c_v')\n",
    "    return coherence_model.get_coherence()\n",
    "\n",
    "# Range of topic numbers to evaluate\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step = 1\n",
    "\n",
    "# List to store coherence scores\n",
    "coherence_scores = []\n",
    "\n",
    "# Calculate coherence scores for the range of topic numbers\n",
    "for num_topics in range(min_topics, max_topics + 1, step):\n",
    "    coherence_score = calculate_coherence_score(num_topics)\n",
    "    coherence_scores.append((num_topics, coherence_score))\n",
    "    print(f'Coherence Score for {num_topics} Topics: {coherence_score}')\n",
    "\n",
    "# Find the optimal number of topics\n",
    "optimal_topics = max(coherence_scores, key=lambda x: x[1])[0]\n",
    "print(f'Optimal Number of Topics: {optimal_topics}')\n",
    "\n",
    "# Plot coherence scores\n",
    "topics, scores = zip(*coherence_scores)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(topics, scores, marker='o', linestyle='-', linewidth=2.5, color='b')  # Thicker line with deep color\n",
    "plt.xlabel('Number of Topics', fontsize=14, fontweight='bold')  # Bold font\n",
    "plt.ylabel('Coherence Score', fontsize=14, fontweight='bold')  # Bold font\n",
    "plt.title('Coherence Scores for Different Number of Topics', fontsize=16, fontweight='bold')  # Bold font\n",
    "plt.xticks(fontsize=12, fontweight='bold')  # Bold font for tick labels\n",
    "plt.yticks(fontsize=12, fontweight='bold')  # Bold font for tick labels\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Perplexity in Latent Dirichlet Allocation (LDA): Estimating the Ideal Number of Topics**\n",
    "Calculating and plotting perplexity values for various topic numbers to determine the optimal number of topics for the LDA model. The log-perplexity score of the LDA model is used to assess how well the model generalizes to previously unseen data. It iterates over a predefined range of topic numbers, calculating the perplexity for each and determining the number of topics with the lowest perplexity. It is a statistical measure that assesses the quality of a probabilistic model by determining how well it predicts a given sample. The method assists in determining the optimal number of topics for an LDA model, resulting in improved generalisation and model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Assume cleaned and tokenized text data is available in CleanedKeyWords\n",
    "CleanedKeyWords = [\n",
    "    ['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['eps', 'user', 'interface', 'system'],\n",
    "    ['system', 'human', 'system', 'eps'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['trees'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey']\n",
    "]\n",
    "\n",
    "KeyWordsDictionary = corpora.Dictionary(CleanedKeyWords)\n",
    "doc_term_matrix = [KeyWordsDictionary.doc2bow(text) for text in CleanedKeyWords]\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(num_topics):\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(doc_term_matrix, num_topics=num_topics, id2word=KeyWordsDictionary, passes=10, random_state=0)\n",
    "    perplexity = lda_model.log_perplexity(doc_term_matrix)\n",
    "    return perplexity\n",
    "\n",
    "# Range of topic numbers to evaluate\n",
    "min_topics = 2\n",
    "max_topics = 10\n",
    "step = 1\n",
    "\n",
    "# List to store perplexity scores\n",
    "perplexity_scores = []\n",
    "\n",
    "# Calculate perplexity scores for the range of topic numbers\n",
    "for num_topics in range(min_topics, max_topics + 1, step):\n",
    "    perplexity_score = calculate_perplexity(num_topics)\n",
    "    perplexity_scores.append((num_topics, perplexity_score))\n",
    "    print(f'Perplexity for {num_topics} Topics: {perplexity_score}')\n",
    "\n",
    "# Find the optimal number of topics based on perplexity (lower is better)\n",
    "optimal_topics = min(perplexity_scores, key=lambda x: x[1])[0]\n",
    "print(f'Optimal Number of Topics based on Perplexity: {optimal_topics}')\n",
    "\n",
    "# Plot perplexity values\n",
    "topics, perplexity_values = zip(*perplexity_scores)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(topics, perplexity_values, marker='o', linestyle='-', linewidth=2.5, color='r')\n",
    "plt.xlabel('Number of Topics', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Perplexity', fontsize=14, fontweight='bold')\n",
    "plt.title('Perplexity for Different Number of Topics', fontsize=16, fontweight='bold')\n",
    "plt.xticks(fontsize=12, fontweight='bold')\n",
    "plt.yticks(fontsize=12, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Latent Dirichlet Allocation (LDA) for Topic Modeling of Key Words**\n",
    "Latent Dirichlet Allocation (LDA) analysis of this dataset's \"KeyWords\" column revealing latent topics represented by the most important keywords. Cleaning the keywords, splitting them into lists, and creating a dictionary and document-term matrix. The matrix used to train an LDA model to extract nine topics with top ten keywords each. The bar charts show the top words for each topic. This machine learning topic modeling using LDA (Latent Dirichlet Allocation) uncovers hidden thematic structures in documents. Helping automatically identify topics in a large set of documents, revealing key themes and aiding data organization, summarization, and comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for LDA analysis\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure CleanedKeyWords is prepared from the \"KeyWords\" column\n",
    "# Clean the KeyWords column\n",
    "corruptnDF['CleanedKeyWords'] = corruptnDF['KeyWords'].apply(lambda x: clean(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Split cleaned keywords into lists\n",
    "CleanedKeyWords = corruptnDF['CleanedKeyWords'].apply(lambda x: x.split())\n",
    "\n",
    "# Creating a dictionary and a bag-of-words matrix for the \"KeyWords\" column\n",
    "KeyWordsDictionary = corpora.Dictionary(CleanedKeyWords)\n",
    "doc_term_matrix_keywords = [KeyWordsDictionary.doc2bow(keyword) for keyword in CleanedKeyWords]\n",
    "\n",
    "# Define an LDA (Latent Dirichlet Allocation) model to discover the top keywords\n",
    "lda = LdaModel\n",
    "\n",
    "# Train the LDA model on the document-term matrix with 8 topics\n",
    "lda_model_keywords = lda(doc_term_matrix_keywords, num_topics=9, id2word=KeyWordsDictionary, passes=10, random_state=0, eval_every=1)\n",
    "\n",
    "# Retrieve the top words representing each of the 9 topics with their weights\n",
    "top_keywords = lda_model_keywords.print_topics(num_words=10)\n",
    "\n",
    "# Print a summary of top keywords for each topic\n",
    "for idx, topic in top_keywords:\n",
    "    print(f\"Topic {idx + 1}: {topic}\")\n",
    "\n",
    "# Visualization of the topic keywords\n",
    "topics = {i: [word for word, prob in lda_model_keywords.show_topic(i, topn=10)] for i in range(9)}\n",
    "weights = {i: [prob for word, prob in lda_model_keywords.show_topic(i, topn=10)] for i in range(9)}\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(24, 18), sharey=True)  # Adjusted the subplot grid for 9 topics and increased figure size\n",
    "axes = axes.flatten()\n",
    "for i in range(9):\n",
    "    ax = axes[i]\n",
    "    ax.bar(topics[i], weights[i], color='lightblue')\n",
    "    ax.set_title(f'Topic {i + 1}', fontsize=18)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=12, rotation=45)\n",
    "\n",
    "fig.suptitle('Top Words per Topic', fontsize=26)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Enhanced Visualisation of Top Keywords in LDA Topic Modeling**\n",
    "Analyses the dataset's \"KeyWords\" column using Latent Dirichlet Allocation (LDA) to identify important topics and visualise their top keywords. Cleaning and processing keywords to create a dictionary and document-term matrix. Nine topics with top keywords are extracted by an LDA model. Visualised in a horizontal bar chart displaying the keywords in descending order by cumulative weight and annotates each bar with its weight percentage. The LDA (Latent Dirichlet Allocation) is a machine learning topic modelling method that uncovers hidden document themes. Its advantage is in automatically finding the main themes in a large set of documents, helping organise, summarise, and understand data.\n",
    "\n",
    "Proceeding to perform a Thematic Analysis of Technology and Corruption Keywords and Risks. Showing the importance of technology, corruption, and risk keywords. Extracting and processing keywords, assigning them to themes, and creating a horizontal bar chart to show their importance. The goal being to highlight techno-corruption and risk terms to help understand their prevalence and thematic connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the topics and weights from the updated output for 9 topics\n",
    "topics_keywords = [\n",
    "    [\"transfer\", \"cash\", \"economic\", \"payment\", \"corruption\", \"impact\", \"vulnerable\", \"jamii\", \"inua\", \"social\"],\n",
    "    [\"corruption\", \"development\", \"technology\", \"sustainable\", \"security\", \"economic\", \"governance\", \"manipulation\", \"control\", \"election\"],\n",
    "    [\"public\", \"corruption\", \"digital\", \"service\", \"transparency\", \"kenya\", \"national\", \"data\", \"huduma\", \"infrastructure\"],\n",
    "    [\"fund\", \"health\", \"social\", \"insurance\", \"service\", \"national\", \"healthcare\", \"process\", \"nhif\", \"economic\"],\n",
    "    [\"social\", \"transfer\", \"cash\", \"protection\", \"programme\", \"vulnerable\", \"national\", \"net\", \"safety\", \"program\"],\n",
    "    [\"corruption\", \"kenya\", \"service\", \"anticorruption\", \"transparency\", \"public\", \"financial\", \"economic\", \"accountability\", \"national\"],\n",
    "    [\"corruption\", \"procurement\", \"fund\", \"fraud\", \"investigation\", \"charge\", \"withdrawal\", \"anticorruption\", \"kenya\", \"eacc\"],\n",
    "    [\"social\", \"corruption\", \"public\", \"kenya\", \"bank\", \"economic\", \"world\", \"poverty\", \"growth\", \"health\"],\n",
    "    [\"corruption\", \"public\", \"political\", \"investigation\", \"financial\", \"george\", \"muchai\", \"assassination\", \"anticorruption\", \"judicial\"]\n",
    "]\n",
    "\n",
    "topics_weights = [\n",
    "    [0.034, 0.032, 0.020, 0.016, 0.016, 0.015, 0.014, 0.012, 0.012, 0.012],\n",
    "    [0.020, 0.018, 0.011, 0.011, 0.011, 0.011, 0.009, 0.008, 0.008, 0.008],\n",
    "    [0.038, 0.036, 0.019, 0.016, 0.016, 0.014, 0.010, 0.010, 0.010, 0.010],\n",
    "    [0.026, 0.018, 0.018, 0.015, 0.010, 0.009, 0.009, 0.009, 0.008, 0.008],\n",
    "    [0.043, 0.041, 0.040, 0.025, 0.023, 0.020, 0.017, 0.017, 0.017, 0.014],\n",
    "    [0.040, 0.020, 0.016, 0.015, 0.015, 0.015, 0.014, 0.012, 0.012, 0.012],\n",
    "    [0.024, 0.018, 0.017, 0.015, 0.013, 0.013, 0.013, 0.013, 0.012, 0.011],\n",
    "    [0.017, 0.016, 0.014, 0.013, 0.012, 0.010, 0.009, 0.009, 0.008, 0.008],\n",
    "    [0.018, 0.014, 0.013, 0.012, 0.011, 0.010, 0.010, 0.010, 0.009, 0.009]\n",
    "]\n",
    "\n",
    "# Combine keywords and their cumulative weights into a list of tuples\n",
    "keyword_weight_tuples = [\n",
    "    (\"transfer\", 0.034 + 0.041),\n",
    "    (\"cash\", 0.032 + 0.040),\n",
    "    (\"economic\", 0.020 + 0.011 + 0.009 + 0.012 + 0.014),\n",
    "    (\"payment\", 0.016),\n",
    "    (\"corruption\", 0.016 + 0.020 + 0.036 + 0.040 + 0.024 + 0.017 + 0.018),\n",
    "    (\"impact\", 0.015),\n",
    "    (\"vulnerable\", 0.014 + 0.023),\n",
    "    (\"jamii\", 0.012),\n",
    "    (\"inua\", 0.012),\n",
    "    (\"social\", 0.012 + 0.018 + 0.043 + 0.041 + 0.017),\n",
    "    (\"development\", 0.018),\n",
    "    (\"technology\", 0.011),\n",
    "    (\"sustainable\", 0.011),\n",
    "    (\"security\", 0.011),\n",
    "    (\"governance\", 0.009),\n",
    "    (\"manipulation\", 0.008),\n",
    "    (\"control\", 0.008),\n",
    "    (\"election\", 0.008),\n",
    "    (\"public\", 0.038 + 0.014 + 0.017 + 0.016 + 0.014 + 0.018),\n",
    "    (\"digital\", 0.019),\n",
    "    (\"service\", 0.016 + 0.015 + 0.010 + 0.015),\n",
    "    (\"transparency\", 0.014 + 0.015),\n",
    "    (\"kenya\", 0.010 + 0.014 + 0.013),\n",
    "    (\"national\", 0.010 + 0.012),\n",
    "    (\"data\", 0.010),\n",
    "    (\"huduma\", 0.010),\n",
    "    (\"infrastructure\", 0.010),\n",
    "    (\"fund\", 0.026 + 0.017),\n",
    "    (\"health\", 0.018),\n",
    "    (\"insurance\", 0.015),\n",
    "    (\"healthcare\", 0.009),\n",
    "    (\"process\", 0.009),\n",
    "    (\"nhif\", 0.008),\n",
    "    (\"protection\", 0.025),\n",
    "    (\"programme\", 0.023),\n",
    "    (\"net\", 0.017),\n",
    "    (\"safety\", 0.017),\n",
    "    (\"program\", 0.014),\n",
    "    (\"anticorruption\", 0.015 + 0.013 + 0.009),\n",
    "    (\"financial\", 0.014 + 0.012 + 0.011),\n",
    "    (\"accountability\", 0.012),\n",
    "    (\"procurement\", 0.018),\n",
    "    (\"fraud\", 0.015),\n",
    "    (\"investigation\", 0.013 + 0.012),\n",
    "    (\"charge\", 0.013),\n",
    "    (\"withdrawal\", 0.013),\n",
    "    (\"eacc\", 0.011),\n",
    "    (\"bank\", 0.012),\n",
    "    (\"world\", 0.009),\n",
    "    (\"poverty\", 0.009),\n",
    "    (\"growth\", 0.008),\n",
    "    (\"political\", 0.013),\n",
    "    (\"george\", 0.010),\n",
    "    (\"muchai\", 0.010),\n",
    "    (\"assassination\", 0.010),\n",
    "    (\"judicial\", 0.009)\n",
    "]\n",
    "\n",
    "# Extract keywords and their cumulative weights from the tuples\n",
    "keywords_heaviest_weights, weights = zip(*keyword_weight_tuples)\n",
    "\n",
    "# Sort the keywords and weights in descending order\n",
    "sorted_indices = np.argsort(weights)[::-1]\n",
    "keywords_heaviest_weights = np.array(keywords_heaviest_weights)[sorted_indices]\n",
    "weights = np.array(weights)[sorted_indices]\n",
    "\n",
    "# Define themes and their associated keywords\n",
    "themes = {\n",
    "    'Technology and Corruption': [\"technology\", \"digital\", \"service\", \"procurement\", \"fraud\", \"manipulation\"],\n",
    "    'Risks and Whistleblowing': [\"assassination\", \"investigation\", \"protection\", \"corruption\"],\n",
    "    'Governance and Accountability': [\"governance\", \"eacc\", \"accountability\", \"transparency\", \"judicial\", \"control\"]\n",
    "}\n",
    "\n",
    "# Assign professional colors to themes using the recommended shades\n",
    "theme_colors = {\n",
    "    'Technology and Corruption': '#3498db',  # moderate blue\n",
    "    'Risks and Whistleblowing': '#e74c3c',  # moderate red\n",
    "    'Governance and Accountability': '#27ae60',  # moderate green\n",
    "    'Other': '#95a5a6'  # medium grey\n",
    "}\n",
    "\n",
    "# Determine the color for each keyword based on its theme\n",
    "colors = []\n",
    "label_colors = []\n",
    "for keyword in keywords_heaviest_weights:\n",
    "    assigned = False\n",
    "    for theme, theme_keywords in themes.items():\n",
    "        if keyword in theme_keywords:\n",
    "            colors.append(theme_colors[theme])\n",
    "            label_colors.append(theme_colors[theme])\n",
    "            assigned = True\n",
    "            break\n",
    "    if not assigned:\n",
    "        colors.append(theme_colors['Other'])\n",
    "        label_colors.append('black')  # Default color for non-themed keywords\n",
    "\n",
    "# Calculate average weight for reference\n",
    "average_weight = np.mean(weights)\n",
    "\n",
    "# Create a horizontal bar chart with outlined bars\n",
    "fig, ax = plt.subplots(figsize=(30, 20))  # Further increase figure size\n",
    "y_pos = np.arange(len(keywords_heaviest_weights))\n",
    "\n",
    "# Draw bars with line color and no fill\n",
    "bars = ax.barh(y_pos, weights, edgecolor=colors, color=colors, height=0.8, linewidth=2)  # Increase bar height and linewidth\n",
    "\n",
    "# Adding a red line for average weight\n",
    "ax.axvline(x=average_weight, color='darkred', linestyle='--', linewidth=4, label=f'Average Weight: {average_weight:.3f}')\n",
    "\n",
    "# Annotate bars with percentage values\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width + 0.002, bar.get_y() + bar.get_height()/2, f'{100 * width/sum(weights):.1f}%', va='center', ha='left', fontsize=22, color='black')  # Increase font size and unbold\n",
    "\n",
    "# Set y-axis labels to keywords in capitalized form and their corresponding colors\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels([])  # Remove y-axis labels\n",
    "\n",
    "# Adjust position of the labels to avoid overlapping with the y-axis\n",
    "for i, (keyword, color) in enumerate(zip(keywords_heaviest_weights, label_colors)):\n",
    "    ax.text(-0.005, i, keyword.capitalize(), fontsize=24, color=color, va='center', ha='right', transform=ax.get_yaxis_transform(), clip_on=False)  # Adjust position and color, align to the y-axis\n",
    "\n",
    "# Label axes and title\n",
    "ax.set_xlabel('Weights', fontsize=26, weight='bold', color='black')\n",
    "ax.set_title('Key Terms and Risks in Techno-Corruption News Analysis', fontsize=36, fontweight='bold', color='black', pad=30)  # Increase font size and visibility\n",
    "\n",
    "# Increase fontsize and bolding of numbers on x-axis\n",
    "plt.xticks(fontsize=24, fontweight='bold', color='black')\n",
    "\n",
    "# Add grid lines and legend\n",
    "ax.grid(True, linestyle='--', which='major', color='grey', alpha=0.5)\n",
    "legend_handles = [plt.Line2D([0], [0], color=theme_colors[theme], lw=4, label=theme) for theme in theme_colors if theme != 'Other']\n",
    "legend = ax.legend(handles=legend_handles, loc='upper right', fontsize=24, frameon=True, framealpha=1, shadow=True, borderpad=1)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('black')\n",
    "frame.set_linewidth(3)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_color('black')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**LDA-Based Identification and Visualization of Key Topics**\n",
    "Using Latent Dirichlet Allocation to identify and visualise key topics in the dataset's \"KeyWords\" column. Processes keywords, trains an LDA model to find important topics, and displays the top keywords for each topic in a clear and concise format. Clarifying the dataset's \"KeyWords\" column's main themes. For each topic, the analysis visualises the top keywords and their relevance to reveal the dataset's structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for LDA analysis\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure CleanedKeyWords is prepared from the \"KeyWords\" column\n",
    "# Clean the KeyWords column\n",
    "corruptnDF['CleanedKeyWords'] = corruptnDF['KeyWords'].apply(lambda x: clean(x) if pd.notnull(x) else '')\n",
    "\n",
    "# Split cleaned keywords into lists\n",
    "CleanedKeyWords = corruptnDF['CleanedKeyWords'].apply(lambda x: x.split())\n",
    "\n",
    "# Creating a dictionary and a bag-of-words matrix for the \"KeyWords\" column\n",
    "KeyWordsDictionary = corpora.Dictionary(CleanedKeyWords)\n",
    "doc_term_matrix_keywords = [KeyWordsDictionary.doc2bow(keyword) for keyword in CleanedKeyWords]\n",
    "\n",
    "# Define an LDA (Latent Dirichlet Allocation) model to discover the top keywords\n",
    "lda = LdaModel\n",
    "\n",
    "# Train the LDA model on the document-term matrix with 9 topics\n",
    "lda_model_keywords = lda(doc_term_matrix_keywords, num_topics=9, id2word=KeyWordsDictionary, passes=10, random_state=0, eval_every=1)\n",
    "\n",
    "# Retrieve the top words representing each of the 9 topics with their weights\n",
    "top_keywords = lda_model_keywords.print_topics(num_words=5)\n",
    "\n",
    "# Print a summary of top keywords for each topic\n",
    "for idx, topic in top_keywords:\n",
    "    print(f\"Topic {idx + 1}: {topic}\")\n",
    "\n",
    "# Visualization of the topic keywords\n",
    "topics = {i: [word for word, prob in lda_model_keywords.show_topic(i, topn=5)] for i in range(9)}\n",
    "weights = {i: [prob for word, prob in lda_model_keywords.show_topic(i, topn=5)] for i in range(9)}\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(24, 18), sharey=True)  # Adjusted the subplot grid for 9 topics and increased figure size\n",
    "axes = axes.flatten()\n",
    "for i in range(9):\n",
    "    ax = axes[i]\n",
    "    ax.bar(topics[i], weights[i], color='lightblue')\n",
    "    ax.set_title(f'Topic {i + 1}', fontsize=18)\n",
    "    ax.grid(True)\n",
    "    ax.tick_params(axis='y', labelsize=14)\n",
    "    ax.tick_params(axis='x', labelsize=12, rotation=45)\n",
    "\n",
    "fig.suptitle('Top Words per Topic', fontsize=26)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Visualisation of Most Significant Keywords in LDA Topic Modeling**\n",
    "Highlighting the most significant keywords from Latent Dirichlet Allocation (LDA) topic modelling analysis. Processes keywords, assigns weights, and creates a vertical bar chart to show their relative importance in the dataset. The LDA topic modelling uses statistics to find abstract topics in documents. It assumes documents and topics are word mixtures. The method enabling the identification and summarising of core themes in this kind of large text datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the topics and weights from the updated output for 9 topics with 5 words each\n",
    "topics_keywords = [\n",
    "    [\"transfer\", \"cash\", \"economic\", \"payment\", \"corruption\"],\n",
    "    [\"corruption\", \"development\", \"technology\", \"sustainable\", \"security\"],\n",
    "    [\"public\", \"corruption\", \"digital\", \"service\", \"transparency\"],\n",
    "    [\"fund\", \"health\", \"social\", \"insurance\", \"service\"],\n",
    "    [\"social\", \"transfer\", \"cash\", \"protection\", \"programme\"],\n",
    "    [\"corruption\", \"kenya\", \"service\", \"anticorruption\", \"transparency\"],\n",
    "    [\"corruption\", \"procurement\", \"fund\", \"fraud\", \"investigation\"],\n",
    "    [\"social\", \"corruption\", \"public\", \"kenya\", \"bank\"],\n",
    "    [\"corruption\", \"public\", \"political\", \"investigation\", \"financial\"]\n",
    "]\n",
    "\n",
    "topics_weights = [\n",
    "    [0.034, 0.032, 0.020, 0.016, 0.016],\n",
    "    [0.020, 0.018, 0.011, 0.011, 0.011],\n",
    "    [0.038, 0.036, 0.019, 0.016, 0.016],\n",
    "    [0.026, 0.018, 0.018, 0.015, 0.010],\n",
    "    [0.043, 0.041, 0.040, 0.025, 0.023],\n",
    "    [0.040, 0.020, 0.016, 0.015, 0.015],\n",
    "    [0.024, 0.018, 0.017, 0.015, 0.013],\n",
    "    [0.017, 0.016, 0.014, 0.013, 0.012],\n",
    "    [0.018, 0.014, 0.013, 0.012, 0.011]\n",
    "]\n",
    "\n",
    "# Combine keywords and their cumulative weights into a list of tuples\n",
    "keyword_weight_tuples = [\n",
    "    (\"transfer\", 0.034 + 0.041),\n",
    "    (\"cash\", 0.032 + 0.040),\n",
    "    (\"economic\", 0.020),\n",
    "    (\"payment\", 0.016),\n",
    "    (\"corruption\", 0.016 + 0.020 + 0.036 + 0.040 + 0.024 + 0.018),\n",
    "    (\"development\", 0.018),\n",
    "    (\"technology\", 0.011),\n",
    "    (\"sustainable\", 0.011),\n",
    "    (\"security\", 0.011),\n",
    "    (\"public\", 0.038 + 0.014 + 0.017 + 0.018),\n",
    "    (\"digital\", 0.019),\n",
    "    (\"service\", 0.016 + 0.015 + 0.010 + 0.015),\n",
    "    (\"transparency\", 0.014 + 0.015),\n",
    "    (\"kenya\", 0.010 + 0.014 + 0.013),\n",
    "    (\"fund\", 0.026 + 0.017),\n",
    "    (\"health\", 0.018),\n",
    "    (\"insurance\", 0.015),\n",
    "    (\"social\", 0.012 + 0.018 + 0.043 + 0.041 + 0.017),\n",
    "    (\"protection\", 0.025),\n",
    "    (\"programme\", 0.023),\n",
    "    (\"procurement\", 0.018),\n",
    "    (\"fraud\", 0.015),\n",
    "    (\"investigation\", 0.013 + 0.012),\n",
    "    (\"bank\", 0.012),\n",
    "    (\"political\", 0.013),\n",
    "    (\"financial\", 0.011)\n",
    "]\n",
    "\n",
    "# Extract keywords and their cumulative weights from the tuples and sort them in descending order\n",
    "keywords_heaviest_weights, weights = zip(*sorted(keyword_weight_tuples, key=lambda x: x[1], reverse=True))\n",
    "\n",
    "# Professional color palette\n",
    "colors = plt.cm.tab20c(np.linspace(0, 1, len(keywords_heaviest_weights)))\n",
    "\n",
    "# Calculate average weight for reference\n",
    "average_weight = np.mean(weights)\n",
    "\n",
    "# Create a vertical bar chart\n",
    "fig, ax = plt.subplots(figsize=(30, 20))  # Adjust figure size\n",
    "x_pos = np.arange(len(keywords_heaviest_weights))\n",
    "bars = ax.bar(x_pos, weights, color=colors, edgecolor='black', width=0.5)  # Adjust bar width\n",
    "\n",
    "# Adding a red line for average weight\n",
    "ax.axhline(y=average_weight, color='darkred', linestyle='--', label=f'Average Weight: {average_weight:.3f}', linewidth=4)\n",
    "\n",
    "# Annotate bars with percentage values, slanted to avoid overlay\n",
    "for bar, weight in zip(bars, weights):\n",
    "    height = bar.get_height()\n",
    "    color = 'red' if weight > average_weight else 'black'\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height + 0.002, f'{100 * height/sum(weights):.1f}%', ha='center', va='bottom', fontsize=28, weight='bold', color=color, rotation=45)\n",
    "\n",
    "# Set x-axis labels to keywords\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([kw.capitalize() for kw in keywords_heaviest_weights], rotation=45, ha='right', fontsize=28, weight='bold')\n",
    "\n",
    "# Label axes and title\n",
    "ax.set_ylabel('Weights', fontsize=32, weight='bold', color='black')\n",
    "ax.set_title('Techno-Corruption: Most Significant Key Words', fontsize=48, fontweight='bold', pad=30, color='black')  # Increase font size and bold\n",
    "\n",
    "# Set y-axis labels font size, bold and darker\n",
    "plt.yticks(fontsize=28, fontweight='bold', color='black')\n",
    "\n",
    "# Enhance the legend box\n",
    "legend = ax.legend(loc='upper right', fontsize=32, frameon=True, framealpha=1, shadow=True, borderpad=1)\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('black')\n",
    "frame.set_linewidth(3)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontweight('bold')\n",
    "    text.set_color('black')\n",
    "\n",
    "# Add grid lines\n",
    "ax.grid(True, linestyle='--', which='major', color='grey', alpha=0.5)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Interactive Visualising of LDA Topic Modeling Results**\n",
    "Preprocesses text data, trains an LDA model with the optimal number of topics, and interactively visualises topics using pyLDAvis. The complehensive preprocessing ensures high-quality data for LDA input. Realising an Optimal Topic Model identifies the most coherent, meaningful topics. Interactive Visualisation simplifies topic exploration, improving understanding and presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Install pyLDAvis if it's not already installed\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Ensure nltk resources are downloaded\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "try:\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(f\"The file {excel_file_path} was not found.\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"An error occurred while loading the file: {e}\")\n",
    "\n",
    "# Preprocessing function for the text\n",
    "def prepare_text_for_lda(text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    custom_stop_words = {'would', 'could', 'however'}\n",
    "    stop_words.update(custom_stop_words)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words and len(token) > 4]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply the preprocessing function to the 'Content' column\n",
    "corruptnDF['tokenized_texts'] = corruptnDF['Content'].dropna().apply(prepare_text_for_lda)\n",
    "\n",
    "# Creating a dictionary and corpus for LDA analysis\n",
    "dictionary = corpora.Dictionary(corruptnDF['tokenized_texts'])\n",
    "corpus = [dictionary.doc2bow(text) for text in corruptnDF['tokenized_texts']]\n",
    "\n",
    "# Validate data integrity\n",
    "if not corpus:\n",
    "    raise ValueError(\"The corpus is empty. Ensure that the corpus is prepared correctly.\")\n",
    "if not dictionary:\n",
    "    raise ValueError(\"The dictionary is empty. Ensure that the dictionary is prepared correctly.\")\n",
    "\n",
    "# Train the final LDA model with the optimal number of topics\n",
    "optimal_num_topics = 9\n",
    "final_lda_model = models.LdaModel(corpus, num_topics=optimal_num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "# Validate the model\n",
    "if not final_lda_model:\n",
    "    raise ValueError(\"LDA model training failed. Please check the input data and parameters.\")\n",
    "\n",
    "# Visualization using pyLDAvis\n",
    "lda_display = gensimvis.prepare(final_lda_model, corpus, dictionary, sort_topics=False)\n",
    "\n",
    "# Display the LDA visualization\n",
    "pyLDAvis.display(lda_display)\n",
    "\n",
    "# Save the visualization for future reference\n",
    "pyLDAvis.save_html(lda_display, 'lda_visualization.html')\n",
    "\n",
    "# Print topics and their top keywords for documentation\n",
    "for idx, topic in final_lda_model.print_topics(num_words=10):\n",
    "    print(f\"Topic {idx + 1}: {topic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Interactive Visualising of LDA Model Results with pyLDAvis**\n",
    "Visualises an already trained LDA model's results using pyLDAvis, an interactive tool that interprets model topics. Interactive topic model visualisation with Python library pyLDAvis helps users intuitively explore and interpret topic modelling results. Enhancing interpretation using pyLDAvis makes it easier to understand the corpus' topic distribution and relationships by exploring LDA model topics in an intuitive and interactive way. It improves complex dataset analysis and interpretation by allowing detailed inspection of the most relevant terms for each topic and the corpus' topic distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Assume the variables `final_lda_model`, `corpus`, and `dictionary` are already defined\n",
    "# Validate the final LDA model\n",
    "if 'final_lda_model' not in globals() or not final_lda_model:\n",
    "    raise ValueError(\"LDA model is not defined or invalid. Please train the model before visualization.\")\n",
    "if 'corpus' not in globals() or not corpus:\n",
    "    raise ValueError(\"Corpus is not defined or invalid. Please prepare the corpus before visualization.\")\n",
    "if 'dictionary' not in globals() or not dictionary:\n",
    "    raise ValueError(\"Dictionary is not defined or invalid. Please prepare the dictionary before visualization.\")\n",
    "\n",
    "# Visualization using pyLDAvis\n",
    "lda_display = gensimvis.prepare(final_lda_model, corpus, dictionary, sort_topics=False)\n",
    "\n",
    "# Display the LDA visualization\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Analysing and Visualising Top Bigrams in Techno-Corruption Discussions**\n",
    "Preprocesses text data, extracts bigrams, and visualises most frequently occurring bigrams techno-corruption discussions. After tokenising, lemmatising, and filtering the text, it generates bigrams and counts their occurrences using natural language processing. The key bigrams are displayed in a bar chart. Identifying and visualising the most discussed techno-corruption bigrams to reveal common themes and focus areas. Highlighting bigrams with frequencies above average helps identify important terms in discussions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                           names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Define a preprocessing function to create processed text from 'Content'\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Advanced text preprocessing to remove punctuation, lowercase the text, lemmatize tokens, and filter out stopwords and specific bigrams.\"\"\"\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['per', 'cent', 'last', 'year', 'moreno', 'ocampo'])  # Adding 'per', 'cent', 'last', 'year', 'moreno', 'ocampo' to stop words\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Preprocess 'Content' column and extract bigrams\n",
    "corruptnDF = corruptnDF.dropna(subset=['Content'])\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].apply(preprocess_text)\n",
    "\n",
    "def extract_ngrams(data, num=2):\n",
    "    \"\"\"Generate n-grams from provided text data.\"\"\"\n",
    "    n_grams = ngrams(word_tokenize(data), num)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Custom synonym mapping for bigrams\n",
    "def apply_synonym_mapping(bigrams):\n",
    "    synonym_map = {\n",
    "        'cash transfers': 'cash transfer',\n",
    "        # Add other synonym mappings as needed\n",
    "    }\n",
    "    return [synonym_map.get(bigram, bigram) for bigram in bigrams]\n",
    "\n",
    "corruptnDF['Bigrams'] = corruptnDF['Processed_Content'].apply(lambda x: apply_synonym_mapping(extract_ngrams(x, 2)))\n",
    "\n",
    "# Aggregate and count occurrences of each bigram\n",
    "all_bigrams = sum(corruptnDF['Bigrams'], [])\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "# Filter out unwanted bigrams\n",
    "unwanted_bigrams = {'last year', 'moreno ocampo', 'covid 19'}\n",
    "bigram_counts = {bigram: count for bigram, count in bigram_counts.items() if bigram not in unwanted_bigrams}\n",
    "\n",
    "# Ensure non-zero bigram counts\n",
    "bigram_counts = {bigram: count for bigram, count in bigram_counts.items() if count > 0}\n",
    "\n",
    "most_common_bigrams = Counter(bigram_counts).most_common(20)\n",
    "\n",
    "# Calculate average frequency\n",
    "average_frequency = sum(count for bigram, count in most_common_bigrams) / len(most_common_bigrams)\n",
    "\n",
    "# Plotting the most common bigrams using Seaborn\n",
    "plt.figure(figsize=(15, 8))\n",
    "ax = sns.barplot(x=[bigram.capitalize() for bigram, count in most_common_bigrams], y=[count for bigram, count in most_common_bigrams], palette='viridis')\n",
    "plt.title('Techno-Corruption Discourse: Top 20 Bigrams', fontsize=24, fontweight='bold')\n",
    "plt.xlabel('Bigrams', fontsize=20, fontweight='bold')\n",
    "plt.ylabel('Frequency', fontsize=20, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Highlighting high-value bigrams above average frequency\n",
    "for p in ax.patches:\n",
    "    if p.get_height() > average_frequency:\n",
    "        p.set_edgecolor('red')\n",
    "        p.set_linewidth(2)\n",
    "\n",
    "# Annotate the average frequency line\n",
    "plt.axhline(average_frequency, color='red', linestyle='--', linewidth=2)\n",
    "plt.text(len(most_common_bigrams) - 1, average_frequency + 10, f'Average: {average_frequency:.1f}', color='red', fontsize=14, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Word Cloud Visualization of Dominant Themes in Techno-Corruption Discussions**\n",
    "Creates a word cloud to illustrate the occurrence of bigrams (consecutive pairs of words) in a dataset of discussions related to techno-corruption. Emphasising the most frequently appearing pairs of words in the text data, enabling a rapid visual recognition of prevailing themes and subjects. Word Cloud Generation is a text mining method that visually displays the frequency of words or phrases in a text dataset. The size of each word in the word cloud corresponds to its frequency or significance. It offers a prompt visual overview of the most common pairs of words in the dataset, making it easier to quickly identify important themes and topics. Thus, helping with exploring the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'bigram_counts' from your n-gram extraction\n",
    "# Convert bigram frequencies from Counter to dictionary\n",
    "bigram_freq_dict = dict(bigram_counts)\n",
    "\n",
    "# Generate a word cloud from bigram frequencies with enhanced visibility\n",
    "wordcloud = WordCloud(\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    background_color='white',\n",
    "    colormap='viridis',\n",
    "    max_font_size=200,\n",
    "    max_words=100\n",
    ").generate_from_frequencies(bigram_freq_dict)\n",
    "\n",
    "# Display the word cloud\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # No axis for word cloud\n",
    "plt.title(\"Techno-Corruption: Main Themes\", fontsize=24, fontweight='bold', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Techno-Corruption: Sentiment Analysis**\n",
    "Tech-corruption text data are preprocessed, bigrams extracted, and sentiment analysed. The outputs include sentiment polarity distribution, most common bigram frequency, and a scatter plot of bigram frequency and average sentiment polarity. The method of text preprocessing uses tokenisation, lemmatisation, and stopword removal to standardise text. NLTK does bigram extraction then searches for and counts pairs of consecutive words (bigrams). TextBlob's is used to calculate sentiment polarity text's positive, neutral, or negative sentiment. Finally, visualisation done using Matplotlib to create histograms, bar charts, and scatter plots for data visualisation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Advanced text preprocessing to remove punctuation, lowercase the text, lemmatize tokens, and filter out stopwords.\"\"\"\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "def extract_ngrams(text, n=2):\n",
    "    \"\"\"Extract n-grams from a given text, excluding common stopwords.\"\"\"\n",
    "    tokens = [word for word in word_tokenize(text.lower()) if word not in stopwords.words('english')]\n",
    "    n_grams = ngrams(tokens, n)\n",
    "    return [' '.join(grams) for grams in n_grams]\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "try:\n",
    "    excel_file_path = '318NewsDataSet.xlsx'\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please upload the file.\")\n",
    "    uploaded = files.upload()\n",
    "    excel_file_path = list(uploaded.keys())[0]\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Preprocess 'Content' column and create 'Processed_Content'\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess_text)\n",
    "\n",
    "# Extract bigrams and create 'Bigrams' column\n",
    "corruptnDF['Bigrams'] = corruptnDF['Processed_Content'].apply(lambda x: extract_ngrams(x, 2))\n",
    "\n",
    "# Perform sentiment analysis and store results\n",
    "corruptnDF['Polarity'] = corruptnDF['Processed_Content'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "\n",
    "# Aggregate and count occurrences of each bigram\n",
    "all_bigrams = sum(corruptnDF['Bigrams'], [])\n",
    "bigram_counts = Counter(all_bigrams)\n",
    "\n",
    "# Filter out unwanted bigrams\n",
    "unwanted_bigrams = {'last year', 'moreno ocampo', 'covid 19', 'per cent'}\n",
    "bigram_counts = {bigram: count for bigram, count in bigram_counts.items() if bigram not in unwanted_bigrams}\n",
    "\n",
    "# Ensure non-zero bigram counts\n",
    "bigram_counts = {bigram: count for bigram, count in bigram_counts.items() if count > 0}\n",
    "\n",
    "# Calculate average frequency\n",
    "most_common_bigrams = Counter(bigram_counts).most_common(20)\n",
    "bigram_labels, bigram_values = zip(*most_common_bigrams)\n",
    "average_frequency = sum(bigram_values) / len(bigram_values)\n",
    "\n",
    "# Visualize polarity distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(corruptnDF['Polarity'].dropna(), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Sentiment Polarity Distribution', fontsize=20, fontweight='bold', color='black')\n",
    "plt.xlabel('Polarity', fontsize=16, fontweight='bold', color='black')\n",
    "plt.ylabel('Frequency', fontsize=16, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=14, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=14, fontweight='bold', color='black')\n",
    "plt.axvline(corruptnDF['Polarity'].mean(), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(corruptnDF['Polarity'].median(), color='green', linestyle='dashed', linewidth=1)\n",
    "plt.legend({'Mean': corruptnDF['Polarity'].mean(), 'Median': corruptnDF['Polarity'].median()})\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the top 20 most common bigrams as well\n",
    "plt.figure(figsize=(15, 8))\n",
    "bars = plt.bar([label.title() for label in bigram_labels], bigram_values, color='skyblue')\n",
    "plt.title('Top 20 Bigrams in Data on Techno-Corrupiton', fontsize=20, fontweight='bold', color='black')\n",
    "plt.xlabel('Bigrams', fontsize=16, fontweight='bold', color='black')\n",
    "plt.ylabel('Frequency', fontsize=16, fontweight='bold', color='black')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=14, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=14, fontweight='bold', color='black')\n",
    "plt.axhline(average_frequency, color='red', linestyle='--', linewidth=2)\n",
    "plt.text(len(bigram_labels) - 1, average_frequency + 10, f'Average: {average_frequency:.1f}', color='red', fontsize=14, ha='center')\n",
    "\n",
    "# Highlighting high-value bigrams above average frequency and annotating frequencies\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    if height > average_frequency:\n",
    "        bar.set_edgecolor('red')\n",
    "        bar.set_linewidth(2)\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, '%d' % int(height), ha='center', va='bottom', fontsize=12, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate average sentiment polarity for each bigram\n",
    "bigram_sentiments = {}\n",
    "for bigram in bigram_counts:\n",
    "    texts_with_bigram = corruptnDF[corruptnDF['Processed_Content'].str.contains(bigram, regex=False)]\n",
    "    bigram_sentiments[bigram] = texts_with_bigram['Polarity'].mean()\n",
    "\n",
    "# Limit the number of points plotted and only label the top 20 bigrams by frequency\n",
    "num_points_to_plot = 50  # Adjust this number as needed\n",
    "most_common_bigrams = Counter(bigram_counts).most_common(num_points_to_plot)\n",
    "bigram_labels, bigram_values = zip(*most_common_bigrams)\n",
    "bigram_polarities = [bigram_sentiments[bigram] for bigram in bigram_labels]\n",
    "\n",
    "# Scatter plot of bigram frequency vs. average sentiment polarity with jitter\n",
    "plt.figure(figsize=(15, 8))\n",
    "x = np.array(bigram_values)\n",
    "y = np.array(bigram_polarities)\n",
    "x_jitter = x + np.random.normal(0, 5, size=len(x))\n",
    "y_jitter = y + np.random.normal(0, 0.001, size=len(y))\n",
    "\n",
    "plt.scatter(x_jitter, y_jitter, color='blue', alpha=0.7, s=100)\n",
    "plt.title('Bigram Frequency vs. Average Sentiment Polarity', fontsize=20, fontweight='bold', color='black')\n",
    "plt.xlabel('Bigram Frequency', fontsize=16, fontweight='bold', color='black')\n",
    "plt.ylabel('Average Sentiment Polarity', fontsize=16, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=14, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=14, fontweight='bold', color='black')\n",
    "plt.grid(True)\n",
    "\n",
    "# Annotating only the top 20 bigrams by frequency\n",
    "for i, txt in enumerate(bigram_labels):\n",
    "    if i < 20:\n",
    "        plt.annotate(txt.title(), (x_jitter[i], y_jitter[i]), fontsize=8, ha='right', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Sentiment Polarity and Subjectivity on Techno-Corruption Text Data**\n",
    "Quantitatively evaluating the emotional tone (polarity) and subjectivity within news data to uncover underlying sentiment trends. The analysis aims to provide insights into the general emotional responses and the degree of objectivity or subjectivity in the reported content. Utilises TextBlob to evaluate and display the sentiment polarity and subjectivity, thus enabling a thorough understanding of the emotional and subjective aspects of this newsdata.\n",
    "Techno-Corruption Sentiment Analysis and Visualisation\n",
    "After reading the dataset and removing missing content, it applies a sentiment analysis function to each content entry. The sentiment polarity and subjectivity scores are shown using histograms. Sentiment Analysis determines text's emotional tone (positive, neutral, negative) and subjectivity (objective to subjective). Polarity measures text positivity or negativity, while subjectivity measures subjective versus objective text. Data visualisation reveals patterns and trends through graphics. Finally, sentiment scores are visualised using histograms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                           names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Ensure there is content to analyze\n",
    "if 'Content' not in corruptnDF.columns:\n",
    "    raise ValueError(\"DataFrame does not contain a 'Content' column\")\n",
    "\n",
    "# Function to calculate sentiment polarity and subjectivity\n",
    "def calculate_sentiment(text):\n",
    "    \"\"\"Calculate sentiment polarity and subjectivity of the provided text.\"\"\"\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "# Preprocess and drop any NaN values in 'Content' column before analysis\n",
    "corruptnDF.dropna(subset=['Content'], inplace=True)\n",
    "\n",
    "# Applying the sentiment analysis function to the 'Content'\n",
    "corruptnDF[['Sentiment_Polarity', 'Sentiment_Subjectivity']] = corruptnDF['Content'].apply(\n",
    "    lambda x: pd.Series(calculate_sentiment(x))\n",
    ")\n",
    "\n",
    "# Print sample of the sentiments being measured\n",
    "print(corruptnDF[['Content', 'Sentiment_Polarity', 'Sentiment_Subjectivity']].head())\n",
    "\n",
    "# Visualizing the sentiment distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(corruptnDF['Sentiment_Polarity'], kde=True, color='#1f77b4', label='Polarity', stat=\"density\", common_norm=False, linewidth=2, alpha=0.8)\n",
    "sns.histplot(corruptnDF['Sentiment_Subjectivity'], kde=True, color='#ff7f0e', label='Subjectivity', stat=\"density\", common_norm=False, linewidth=2, alpha=0.8)\n",
    "\n",
    "# Enhancing the legibility and professional appearance\n",
    "plt.title('Sentiment Analysis of Content', fontsize=20, fontweight='bold', color='black')\n",
    "plt.xlabel('Score', fontsize=16, fontweight='bold', color='black')\n",
    "plt.ylabel('Density', fontsize=16, fontweight='bold', color='black')\n",
    "plt.xticks(fontsize=14, fontweight='bold', color='black')\n",
    "plt.yticks(fontsize=14, fontweight='bold', color='black')\n",
    "\n",
    "# Adding mean lines for better understanding\n",
    "mean_polarity = corruptnDF['Sentiment_Polarity'].mean()\n",
    "mean_subjectivity = corruptnDF['Sentiment_Subjectivity'].mean()\n",
    "\n",
    "plt.axvline(mean_polarity, color='blue', linestyle='--', linewidth=2, label=f'Average Polarity: {mean_polarity:.2f}')\n",
    "plt.axvline(mean_subjectivity, color='green', linestyle='--', linewidth=2, label=f'Average Subjectivity: {mean_subjectivity:.2f}')\n",
    "\n",
    "# Adjusting the legend position to avoid obscuring the histograms\n",
    "legend = plt.legend(loc='upper left', fontsize=14, title='Sentiment', title_fontsize='16')\n",
    "legend.get_frame().set_alpha(0)  # Make the legend box transparent\n",
    "legend.get_frame().set_edgecolor('none')  # Remove the border\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Annotating mean lines\n",
    "plt.annotate(f'{mean_polarity:.2f}', xy=(mean_polarity, 0), xytext=(mean_polarity + 0.05, 0.02),\n",
    "             arrowprops=dict(facecolor='blue', shrink=0.05), fontsize=12, fontweight='bold', color='blue')\n",
    "plt.annotate(f'{mean_subjectivity:.2f}', xy=(mean_subjectivity, 0), xytext=(mean_subjectivity + 0.05, 0.02),\n",
    "             arrowprops=dict(facecolor='green', shrink=0.05), fontsize=12, fontweight='bold', color='green')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**EACC Act and UNCAC Corruption Types Comparative Analysis**\n",
    "Preprocesses and analyses news article text data to determine EACC Act and UNCAC corruption type prevalence. Turning the text into lowercase, removing punctuation, tokenizing, stopwords, and lemmatising text. Then calculates frequency and percentage of EACC Act and UN CAC corruption keywords. Concludes by comparing corruption types across both corruption definitions using pie charts. Preprocessing text cleans and uniformises text data to improve keyword extraction. Keyword Extraction quantifies corruption types in the dataset. Data Visualisation shows corruption types, making data comparison and interpretation easy. Then creates pie charts comparing EACC Act and UN CAC corruption types.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Download required NLTK data\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset with error handling\n",
    "excel_file_path = '318NewsDataSet.xlsx'  # Adjust the path to your dataset\n",
    "try:\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Define the EACC corruption types\n",
    "eacc_corruption_types = {\n",
    "    \"Bribery\": [\"bribe\", \"kickback\", \"illegal payment\", \"cash\", \"inducement\", \"facilitation\", \"brown envelope\", \"handout\"],\n",
    "    \"Embezzlement\": [\"embezzle\", \"misappropriation\", \"theft of funds\", \"pilferage\", \"eacc\", \"theft\", \"siphoning\", \"diversion of funds\", \"appropriation\", \"misallocation\"],\n",
    "    \"Fraud\": [\"fraud\", \"deception\", \"false representation\", \"scam\", \"swindle\", \"cheat\", \"hoax\", \"con\", \"double dealing\", \"falsification\", \"deceit\", \"duplicitous\"],\n",
    "    \"Abuse of Office\": [\"abuse\", \"favoring contractors\", \"personal gain\"],\n",
    "    \"Conflict of Interest\": [\"conflict of interest\", \"appointments\", \"self-dealing\", \"insider trading\", \"unethical practice\", \"coercion\", \"partiality\", \"bias\", \"personal\"],\n",
    "    \"Nepotism\": [\"nepotism\", \"favoritism\", \"family employment\", \"patronage\", \"cronyism\", \"kinship\", \"familial favoritism\", \"preferential hiring\", \"blood relation preference\"],\n",
    "    \"Favouritism\": [\"favoritism\", \"patronage\", \"preferential treatment\", \"biased hiring\"],\n",
    "    \"Extortion\": [\"extort\", \"blackmail\", \"shakedown\", \"ransom\", \"extraction\", \"oppression\", \"intimidation\", \"force\", \"threat\"]\n",
    "}\n",
    "\n",
    "# Define the UN CAC corruption types\n",
    "un_cac_corruption_types = {\n",
    "    \"Bribery of National Public Officials\": [\"bribe\", \"kickback\", \"illegal payment\", \"cash\", \"inducement\", \"facilitation\", \"brown envelope\", \"handout\"],\n",
    "    \"Bribery of Foreign Public Officials\": [\"bribe\", \"foreign official\", \"international kickback\", \"foreign payment\", \"budget corruption\", \"donors\", \"kickback\"],\n",
    "    \"Embezzlement, Misappropriation\": [\"embezzle\", \"misappropriation\", \"theft of funds\", \"pilferage\", \"siphoning\", \"diversion of funds\", \"appropriation\", \"misallocation\", \"cash\"],\n",
    "    \"Trading in Influence\": [\"influence peddling\", \"lobbying for personal gain\", \"undue influence\", \"leverage\", \"exploitation\", \"manipulation\", \"trading influence\", \"power brokering\", \"pulling strings\"],\n",
    "    \"Abuse of Functions\": [\"abuse\", \"favoring contractors\", \"personal gain\", \"abuse of power\"],\n",
    "    \"Illicit Enrichment\": [\"illicit gain\", \"unexplained wealth\", \"corrupt enrichment\", \"illicit enrichment\", \"illegal payment\", \"fraud\", \"inducement\"],\n",
    "    \"Laundering Proceeds of Crime\": [\"money laundering\", \"criminal activity\", \"legal funds\", \"laundering\"],\n",
    "    \"Concealment\": [\"concealment\", \"hiding assets\", \"obfuscation\", \"cover-up\", \"suppression\", \"ghosts\", \"fake\", \"forgery\"],\n",
    "    \"Obstruction of Justice\": [\"obstruction\", \"justice obstruction\", \"interference\", \"witness tampering\", \"evidence tampering\", \"odpp\"]\n",
    "}\n",
    "\n",
    "# Initialize necessary components for text preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text data by converting to lower case, removing punctuation, tokenizing, removing stopwords, and lemmatizing.\n",
    "    Args:\n",
    "        text (str): Input text data.\n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    words = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Content' column to create 'Processed_Content'\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess_text)\n",
    "\n",
    "def extract_keywords(data, corruption_dict):\n",
    "    \"\"\"\n",
    "    Extract keywords and their frequencies from provided text data.\n",
    "    Args:\n",
    "        data (list): List of processed text data.\n",
    "        corruption_dict (dict): Dictionary of corruption types and associated keywords.\n",
    "    Returns:\n",
    "        Counter: Counter object with keyword frequencies.\n",
    "    \"\"\"\n",
    "    keyword_counter = Counter()\n",
    "    for content in data:\n",
    "        words = set(content.split())\n",
    "        for corruption, keywords in corruption_dict.items():\n",
    "            keyword_counter[corruption] += sum(1 for keyword in keywords if keyword in words)\n",
    "    return keyword_counter\n",
    "\n",
    "# Extracting keywords for EACC\n",
    "eacc_keyword_frequencies = extract_keywords(corruptnDF['Processed_Content'], eacc_corruption_types)\n",
    "total_eacc_keywords = sum(eacc_keyword_frequencies.values())\n",
    "eacc_percentages = {corruption: (count / total_eacc_keywords * 100) for corruption, count in eacc_keyword_frequencies.items()} if total_eacc_keywords > 0 else {}\n",
    "\n",
    "# Extracting keywords for UN CAC\n",
    "un_cac_keyword_frequencies = extract_keywords(corruptnDF['Processed_Content'], un_cac_corruption_types)\n",
    "total_un_cac_keywords = sum(un_cac_keyword_frequencies.values())\n",
    "un_cac_percentages = {corruption: (count / total_un_cac_keywords * 100) for corruption, count in un_cac_keyword_frequencies.items()} if total_un_cac_keywords > 0 else {}\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 12), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "# Function to format labels\n",
    "def func(pct, _):\n",
    "    return \"{:.1f}%\".format(pct)\n",
    "\n",
    "# Pie chart for EACC\n",
    "eacc_labels = list(eacc_percentages.keys())\n",
    "eacc_sizes = list(eacc_percentages.values())\n",
    "eacc_explode = [0.1 if size == max(eacc_sizes) else 0 for size in eacc_sizes]  # Explode the largest slice\n",
    "\n",
    "wedges1, texts1, autotexts1 = ax1.pie(eacc_sizes, explode=eacc_explode, labels=eacc_labels, autopct=lambda pct: func(pct, eacc_sizes),\n",
    "                                      shadow=True, startangle=140, textprops={'fontsize': 12, 'fontweight': 'bold', 'color': 'black'})\n",
    "\n",
    "# Pie chart for UN CAC\n",
    "un_cac_labels = list(un_cac_percentages.keys())\n",
    "un_cac_sizes = list(un_cac_percentages.values())\n",
    "un_cac_explode = [0.1 if size == max(un_cac_sizes) else 0 for size in un_cac_sizes]  # Explode the largest slice\n",
    "\n",
    "wedges2, texts2, autotexts2 = ax2.pie(un_cac_sizes, explode=un_cac_explode, labels=un_cac_labels, autopct=lambda pct: func(pct, un_cac_sizes),\n",
    "                                      shadow=True, startangle=140, textprops={'fontsize': 12, 'fontweight': 'bold', 'color': 'black'})\n",
    "\n",
    "# Adjust the percentage labels position\n",
    "for autotext, wedge in zip(autotexts1, wedges1):\n",
    "    angle = (wedge.theta2 - wedge.theta1) / 2. + wedge.theta1\n",
    "    x = wedge.r * 0.8 * np.cos(np.deg2rad(angle))\n",
    "    y = wedge.r * 0.8 * np.sin(np.deg2rad(angle))\n",
    "    if abs(x) < 0.2:  # Adjust label for narrow slices\n",
    "        autotext.set_rotation(90)\n",
    "    autotext.set_position((x, y))\n",
    "\n",
    "for autotext, wedge in zip(autotexts2, wedges2):\n",
    "    angle = (wedge.theta2 - wedge.theta1) / 2. + wedge.theta1\n",
    "    x = wedge.r * 0.8 * np.cos(np.deg2rad(angle))\n",
    "    y = wedge.r * 0.8 * np.sin(np.deg2rad(angle))\n",
    "    if abs(x) < 0.2:  # Adjust label for narrow slices\n",
    "        autotext.set_rotation(90)\n",
    "    autotext.set_position((x, y))\n",
    "\n",
    "# Set overall title closer to the pie charts and with larger font size\n",
    "plt.suptitle('EACC ACT & UNCAC Corruption Types: Comparative Analysis', fontsize=30, fontweight='bold', y=0.92)\n",
    "\n",
    "ax1.set_title('EACC Corruption Types', fontsize=20, fontweight='bold')\n",
    "ax2.set_title('UN CAC Corruption Types', fontsize=20, fontweight='bold')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Frequency Distribution of Conventional Corruption Types**\n",
    "Visualises the frequency of conventional corruption types in news articles. Loads the dataset and preprocess the text data by converting it to lowercase, removing punctuation, tokenizing, stopwords, and lemmatizing. Research-based keywords related to predefined conventional corruption types are extracted and their frequencies and percentages calculated. Finally, displaying these percentages in a horizontal bar chart with a vertical line for the average. The method of keyword extraction quantifies conventional corruption prevalence. Data visualisation shows the distribution of those conventional corruption types and visualises the most and least common in a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset with error handling\n",
    "excel_file_path = '318NewsDataSet.xlsx'  # Adjust the path to your dataset\n",
    "try:\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Define the corruption types and their associated keywords and phrases\n",
    "corruption_type = {\n",
    "    \"Bribery\": [\"bribe\", \"kickback\", \"illegal payment\", \"cash\", \"inducement\", \"facilitation\", \"brown envelope\", \"handout\"],\n",
    "    \"Embezzlement\": [\"embezzle\", \"misappropriation\", \"theft of funds\", \"pilferage\", \"eacc\", \"theft\", \"siphoning\", \"diversion of funds\", \"appropriation\", \"misallocation\"],\n",
    "    \"Fraud\": [\"fraud\", \"deception\", \"false representation\", \"scam\", \"swindle\", \"cheat\", \"hoax\", \"con\", \"double dealing\", \"falsification\", \"deceit\", \"duplicitous\"],\n",
    "    \"Extortion\": [\"extort\", \"blackmail\", \"shakedown\", \"ransom\", \"extraction\", \"oppression\", \"intimidation\", \"force\", \"threat\"],\n",
    "    \"Kickbacks\": [\"kickback\", \"illegal commission\", \"rebate\", \"payoff\", \"secret commission\", \"percentage\", \"finders fee\"],\n",
    "    \"Whistleblower Suppression\": [\"whistleblower\", \"informant\", \"reporting misconduct\", \"whistleblowing\", \"whistle blower\", \"tipster\", \"leaker\", \"complainant\", \"disclosure\"],\n",
    "    \"Nepotism\": [\"nepotism\", \"favoritism\", \"family employment\", \"patronage\", \"cronyism\", \"kinship\", \"familial favoritism\", \"preferential hiring\", \"blood relation preference\"],\n",
    "    \"Cronyism\": [\"cronyism\", \"friendship favoritism\", \"old boy network\", \"favoritism\", \"nepotism\", \"buddy system\", \"chumocracy\", \"clique\"],\n",
    "    \"Patronage\": [\"patronage\", \"political appointments\", \"favoritism\", \"sponsorship\", \"clientelism\", \"spoils system\", \"political spoils\", \"preferential treatment\"],\n",
    "    \"Graft\": [\"graft\", \"bribery\", \"illicit gain\", \"venality\", \"corrupt practices\", \"illicit enrichment\", \"payoff\", \"grease\"],\n",
    "    \"Influence Peddling\": [\"influence peddling\", \"lobbying for personal gain\", \"undue influence\", \"leverage\", \"exploitation\", \"manipulation\", \"trading influence\", \"power brokering\", \"pulling strings\"],\n",
    "    \"Misappropriation\": [\"misappropriate\", \"misuse of funds\", \"theft\", \"conversion\", \"diversion\", \"misapplication\", \"embezzlement\", \"appropriation\"],\n",
    "    \"Collusion\": [\"collusion\", \"conspiracy\", \"secret agreement\", \"complicity\", \"connivance\", \"plot\", \"scheme\", \"cartel\"],\n",
    "    \"Conflict of Interest\": [\"conflict of interest\", \"appointments\", \"self-dealing\", \"insider trading\", \"unethical practice\", \"coercion\", \"partiality\", \"bias\", \"personal\"],\n",
    "    \"State Capture\": [\"state capture\", \"ifmis\", \"cabinet secretary\", \"political domination\", \"regulatory capture\", \"influence over state\", \"policy manipulation\", \"government manipulation\", \"state influence\"],\n",
    "    \"Budgeted Corruption\": [\"tenderpreneur\", \"procurement\", \"loot\", \"nys\", \"ifmis\", \"kabura\", \"digital financial systems\", \"big data and analytics\", \"state capture\", \"waiguru\",\n",
    "                                           \"theft\", \"misallocation of resources\", \"africog\", \"banks\", \"air\", \"ghost\"]\n",
    "}\n",
    "\n",
    "# Initialize necessary components outside the function to avoid redundancy\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess text data by converting to lower case, removing punctuation, tokenizing, removing stopwords, and lemmatizing.\n",
    "    Args:\n",
    "        text (str): Input text data.\n",
    "    Returns:\n",
    "        str: Preprocessed text.\n",
    "    \"\"\"\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    words = [lemmatizer.lemmatize(word) for word in word_tokenize(text) if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing to the 'Content' column to create 'Processed_Content'\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess_text)\n",
    "\n",
    "def extract_keywords(data, corruption_dict):\n",
    "    \"\"\"\n",
    "    Extract keywords and their frequencies from provided text data.\n",
    "    Args:\n",
    "        data (list): List of processed text data.\n",
    "        corruption_dict (dict): Dictionary of corruption types and associated keywords.\n",
    "    Returns:\n",
    "        Counter: Counter object with keyword frequencies.\n",
    "    \"\"\"\n",
    "    keyword_counter = Counter()\n",
    "    for content in data:\n",
    "        words = set(content.split())\n",
    "        for corruption, keywords in corruption_dict.items():\n",
    "            keyword_counter[corruption] += sum(1 for keyword in keywords if keyword in words)\n",
    "    return keyword_counter\n",
    "\n",
    "# Extracting keywords from the 'Processed_Content'\n",
    "keyword_frequencies = extract_keywords(corruptnDF['Processed_Content'], corruption_type)\n",
    "\n",
    "# Calculate total keywords to handle division by zero if needed\n",
    "total_keywords = sum(keyword_frequencies.values())\n",
    "if total_keywords == 0:\n",
    "    print(\"No keywords to display, dataset might be empty.\")\n",
    "else:\n",
    "    keyword_percentages = {corruption: (count / total_keywords * 100) for corruption, count in keyword_frequencies.items()}\n",
    "    keyword_percentages = dict(sorted(keyword_percentages.items(), key=lambda item: item[1], reverse=True))  # Sort the percentages\n",
    "\n",
    "    # Calculate the average percentage\n",
    "    average_percentage = sum(keyword_percentages.values()) / len(keyword_percentages)\n",
    "\n",
    "    # Plotting the frequency and percentage of corruption types using a horizontal bar chart\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    bars = plt.barh(list(keyword_percentages.keys()), list(keyword_percentages.values()), color='darkblue')\n",
    "    plt.axvline(x=average_percentage, color='red', linestyle='--', label=f'Average: {average_percentage:.2f}%')\n",
    "    plt.title('Research-Based Conventional Corruption Types', fontsize=20, fontweight='bold', color='black')\n",
    "    plt.ylabel('Corruption Types', fontsize=16, fontweight='bold', color='black')\n",
    "    plt.xlabel('Percentage (%)', fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "    # Adding text labels to each bar for better clarity and highlight small values in red\n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        annotation_color = 'red' if width < 1 else 'black'\n",
    "        plt.annotate(f'{width:.2f}%', xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                     xytext=(3, 0), textcoords=\"offset points\", ha='left', va='center',\n",
    "                     fontsize=12, color=annotation_color, fontweight='bold')\n",
    "\n",
    "    # Adjust y-axis tick labels for better visibility\n",
    "    plt.yticks(fontsize=14, fontweight='bold')\n",
    "\n",
    "    plt.legend(loc='upper right', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Government-Recognised Corruption Types in Digital and E-Governance**\n",
    "Analyses the frequency and percentage of government-recognised corruption types in the digital and e-government era. It preprocesses news article text data, counts corruption type keywords, calculates their frequency and percentage distribution, and plots the results in a bar plot. This is done using text mining and data visualisation. It involves text preprocessing, lowercase conversion, punctuation removal, tokenizing, stopword removal, and lemmatizing. Regular expressions are used to count corruption-related keywords in the processed text. The frequency and percentage of each corruption type are calculated by summing keyword occurrences. Frequency distribution of corruption types is visualised using a bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Load dataset with error handling\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "try:\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Government recognized \"corruption types\" in the present transformed digital and e-government era\n",
    "corruption_types = {\n",
    "    'Transaction Fraud': ['unauthorized transaction', 'hacking', 'financial systems', 'theft'],\n",
    "    'Public Corruption': ['bribery', 'embezzlement', 'nepotism'],\n",
    "    'Digital Fraud': ['digital fraud', 'digital lockers', 'personal data'],\n",
    "    'Service Delivery Corruption': ['bribery', 'favoritism', 'service delivery'],\n",
    "    'Land Registry Fraud': ['land registration', 'illegal land transfers', 'forgery', 'falsification of documents'],\n",
    "    'Supply Chain Fraud': ['supply chain', 'counterfeit goods', 'document forgery'],\n",
    "    'Bribery': ['bribery', 'offering value', 'receiving value'],\n",
    "    'Embezzlement': ['embezzlement', 'misappropriation', 'diverted funds'],\n",
    "    'Nepotism': ['nepotism', 'favoritism', 'relatives'],\n",
    "    'Tax Evasion': ['tax evasion', 'underpayment of tax', 'non-payment of tax'],\n",
    "    'Counterfeit Goods': ['counterfeit goods', 'unauthorized manufacturing', 'distribution'],\n",
    "    'Vote Tampering': ['vote tampering', 'election interference', 'vote tallying'],\n",
    "    'Misuse Public Office': ['abuse', 'favoring contractors', 'personal gain', 'nys'],\n",
    "    'Fraudulent Documentation': ['forgery', 'false documents', 'deception', 'fake'],\n",
    "    'Money Laundering': ['money laundering', 'criminal activity', 'legal funds'],\n",
    "    'Procurement Fraud': ['ghost', 'procurement fraud', 'false claims', 'procurement contract', 'ifmis'],\n",
    "    'Cyber Fraud': ['cyber fraud', 'internet services', 'software']\n",
    "}\n",
    "\n",
    "# Initialize necessary components for text preprocessing\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocess text data by converting to lower case, removing punctuation, tokenizing, removing stopwords, and lemmatizing.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the Content column\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess)\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    \"\"\"Count keyword occurrences in text using regex.\"\"\"\n",
    "    return sum(len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text)) for keyword in keywords)\n",
    "\n",
    "# Initialize frequency dictionary\n",
    "frequency = {term: 0 for term in corruption_types}\n",
    "\n",
    "# Calculate the frequency of each corruption type\n",
    "for _, row in corruptnDF.iterrows():\n",
    "    processed_content = row['Processed_Content']\n",
    "    for term, keywords in corruption_types.items():\n",
    "        frequency[term] += count_keywords(processed_content, keywords)\n",
    "\n",
    "# Calculate the percentage frequency\n",
    "total_mentions = sum(frequency.values())\n",
    "percentage_frequency = {term: (count / total_mentions) * 100 if total_mentions > 0 else 0 for term, count in frequency.items()}\n",
    "\n",
    "# Convert to DataFrame for visualization and sort in descending order\n",
    "freq_df = pd.DataFrame(list(percentage_frequency.items()), columns=['Corruption Type', 'Percentage'])\n",
    "freq_df = freq_df.sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = sum(freq_df['Percentage']) / len(freq_df)\n",
    "\n",
    "# Enhanced Visualization with a Different Color Scheme and Average Line\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x='Percentage', y='Corruption Type', data=freq_df, palette='viridis')\n",
    "ax.set_xlabel('Percentage (%)', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Corruption Types', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_title('Corruption Types Identified By Government TaskForce', fontsize=28, fontweight='bold', color='black', pad=20)\n",
    "\n",
    "# Add average line\n",
    "plt.axvline(x=average_percentage, color='darkred', linestyle='--', label=f'Average: {average_percentage:.2f}%', linewidth=3)\n",
    "\n",
    "# Enhance y-axis labels\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_fontsize(16)\n",
    "    label.set_fontweight('bold')\n",
    "    label.set_color('black')\n",
    "\n",
    "# Increase and bold x-axis numbers\n",
    "plt.xticks(fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Adding percentage labels\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.2f%%', label_type='edge', fontsize=14, color='black')  # Adjust font size and color\n",
    "\n",
    "# Enhance the legend\n",
    "plt.legend(fontsize=18, loc='lower right', frameon=True, framealpha=1, shadow=True, borderpad=1)\n",
    "legend = ax.legend()\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('black')\n",
    "frame.set_linewidth(2)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize(14)\n",
    "    text.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Research-Based Techno-Corruption in Digital and E-Governance: Identified Using Text Mining**\n",
    "Analyses and visualises techno-corruption from a news article dataset using text mining. As a method, text mining has been used to identify, calculate, and visualise the most common corruption types in the dataset. It enables the extraction of useful information from text data. NLP performs information retrieval, text classification, clustering, sentiment analysis, and entity extraction. The method accurately captures and quantifies techno-corruption, and show its most common forms. This demonstrates how technology and corruption interact and identifies areas for innovative digital anti-corruption measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Load the dataset\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                           names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Techno-corruption types and their keywords according to research\n",
    "techno_corruption_types = {\n",
    "    \"Digital Ledger Technology (DLT) Corruption\": [\"blockchain\", \"distributed ledger\", \"decentralized\", \"public records\", \"peer-to-peer\", \"authenticity\", \"transparency\", \"traceability\", \"accountability\", \"immutable records\", \"fraud prevention\"],\n",
    "    \"Artificial Intelligence (AI) Corruption\": [\"ai\", \"corruption prevention\", \"security\", \"integrity\", \"reliability\", \"election disputes\"],\n",
    "    \"State Capture via Technology\": [\"state capture\", \"technology\", \"influence\", \"policies\", \"legal environment\", \"private interests\", \"powerful individuals\", \"institutions\", \"companies\", \"groups\", \"shaping policies\"],\n",
    "    \"Cyber Corruption\": [\"cyber\", \"digital platforms\", \"hacking\", \"manipulation\", \"digital data\", \"internet\", \"cybersecurity\", \"hacking prevention\", \"encrypted messages\", \"secure transactions\"],\n",
    "    \"Digital Identity Fraud\": [\"digital identity\", \"fraud\", \"corruption\", \"unauthorized access\", \"government services\", \"identity theft\", \"digital id\", \"personal identification\", \"unique identifier\"],\n",
    "    \"Biometric Data Misuse\": [\"biometric data\", \"misuse\", \"transparency\", \"fraud\", \"corruption\", \"compromise\", \"security\"],\n",
    "    \"Electronic Voting System Manipulation\": [\"electronic voting\", \"voter fraud\", \"tamper-proof\", \"election results\", \"security\", \"monitoring\", \"integrity\"],\n",
    "    \"Corruption in Digital Public Infrastructure (DPI)\": [\"digital public infrastructure\", \"dpi\", \"transparency\", \"security\", \"india stack\", \"direct payments\", \"reduce corruption\", \"accountability\"],\n",
    "    \"Digital Bribery\": [\"digital bribery\", \"passport issuance\", \"overcharging\", \"corruption\", \"bribe\", \"ecitizen platform\", \"immigration department\"],\n",
    "    \"Electronic Procurement Fraud\": [\"electronic procurement\", \"fraud\", \"corruption\", \"fictitious employees\", \"manual payroll systems\", \"ippd\", \"implementation\", \"oversight\", \"digital systems\", \"contracts\", \"payments\"],\n",
    "    \"Cryptocurrency-Based Corruption\": [\"cryptocurrency\", \"blockchain\", \"financial systems\", \"transparency\", \"digital currencies\", \"illegal activities\", \"anonymity\", \"regulation\"],\n",
    "    \"Smart Contract Exploitation\": [\"smart contracts\", \"digital data\", \"blockchain law\", \"legal contracts\", \"efficiency\", \"security\"],\n",
    "    \"Social Media Manipulation\": [\"social media\", \"mainstream media\", \"corruption\", \"transparency international\", \"bribery index\", \"digital technology\", \"apuncac\"],\n",
    "    \"Cyber Espionage and Sabotage\": [\"cyber espionage\", \"sabotage\", \"technology\", \"intelligence\", \"state actors\", \"critical infrastructure\", \"extremist groups\", \"cybersecurity\", \"international cooperation\", \"system manipulation\", \"blockchain\", \"ai\"],\n",
    "    \"Insider Threats through Technology\": [\"insider threats\", \"public officer\", \"corruption\", \"infiltrating organizations\", \"graft\", \"financing terrorism\"],\n",
    "    \"Data Manipulation\": [\"data manipulation\", \"corruption\", \"audit\", \"false documents\", \"valuations\", \"immutable technology\", \"fraud\"],\n",
    "    \"Automated Corruption\": [\"automated corruption\", \"ai\", \"blockchain\", \"digital platforms\", \"fraud\", \"illegal activities\", \"public administration\", \"human errors\", \"monitoring\", \"regulatory frameworks\"],\n",
    "    \"Blockchain System Vulnerabilities\": [\"blockchain\", \"vulnerabilities\", \"hackers\", \"decentralized\", \"tamper-proof\", \"implementation\", \"security\", \"malicious actors\", \"system integrity\", \"updates\", \"testing\"],\n",
    "    \"Digital Identity Theft for Corruption\": [\"digital identity theft\", \"corruption\", \"digital id\", \"huduma namba\", \"biometric data\", \"suppression\", \"financial losses\", \"reputational damage\", \"security measures\", \"monitoring\"],\n",
    "    \"Electronic Funds Transfer (EFT) Fraud\": [\"electronic funds transfer\", \"eft\", \"fraud\", \"digital payments\", \"financial transparency\", \"monitoring\", \"security\", \"digital payment systems\", \"fund diversion\", \"financial transactions\"],\n",
    "    \"Encrypted Communications for Corruption\": [\"euro bond\", \"cash transfer\", \"concealed\", \"deep fake\", \"fraud\", \"cooking\", \"hacking\", \"digital extortion\", \"bit coin\", \"unauthorized access\", \"phishing\", \"cyber espionage\", \"cryptocurrencies\", \"electronic communication tools\", \"malware\", \"ransomware\"],\n",
    "    \"Budgeted Corruption via Technology\": [\"tenderpreneur\", \"procurement\", \"corruption\", \"loot\", \"nys\", \"ifmis\", \"kabura\", \"digital financial systems\", \"big data and analytics\", \"state capture\", \"waiguru\", \"theft\", \"misallocation of resources\", \"africog\", \"banks\", \"air\", \"ghost\"],\n",
    "    \"E-Governance Corrupt Appointments\": [\"appointment\", \"nepotism\", \"unethical\", \"court case withdrawn\", \"cleared\", \"charges withdrawn\", \"odpp\", \"cabinet secretary\", \"deputy president\", \"cuomo\", \"rigathe\", \"ti\", \"khrc\", \"eacc\"]\n",
    "}\n",
    "\n",
    "# Initialize necessary components for text preprocessing\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocess text function\n",
    "def preprocess(text):\n",
    "    text = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the Content column\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess)\n",
    "\n",
    "# Function to count keyword occurrences using regex\n",
    "def count_keywords(text, keywords):\n",
    "    return sum(len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text)) for keyword in keywords)\n",
    "\n",
    "# Initialize frequency dictionary\n",
    "frequency = {term: 0 for term in techno_corruption_types}\n",
    "\n",
    "# Calculate the frequency of each corruption type\n",
    "for _, row in corruptnDF.iterrows():\n",
    "    processed_content = row['Processed_Content']\n",
    "    for term, keywords in techno_corruption_types.items():\n",
    "        frequency[term] += count_keywords(processed_content, keywords)\n",
    "\n",
    "# Calculate the percentage frequency\n",
    "total_mentions = sum(frequency.values())\n",
    "percentage_frequency = {term: (count / total_mentions) * 100 for term, count in frequency.items()} if total_mentions > 0 else {term: 0 for term in frequency}\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "freq_df = pd.DataFrame(list(percentage_frequency.items()), columns=['Terminology', 'Percentage'])\n",
    "freq_df = freq_df.sort_values(by='Percentage', ascending=True)\n",
    "\n",
    "# Enhanced Visualization with a Professional Color Scheme using Seaborn\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x='Percentage', y='Terminology', data=freq_df, palette='magma')  # Changed color palette\n",
    "\n",
    "# Adding the average line\n",
    "average = freq_df['Percentage'].mean()\n",
    "plt.axvline(average, color='darkred', linestyle='--', linewidth=2, label=f'Average: {average:.2f}%')\n",
    "\n",
    "ax.set_xlabel('Percentage (%)', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Techno-Corruption Types', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_title('Research-Based Techno-Corruption Types', fontsize=28, fontweight='bold', color='black', pad=20)\n",
    "\n",
    "# Enhance y-axis labels\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_fontsize(16)  # Slightly increased font size\n",
    "    label.set_fontweight('bold')  # Make bold\n",
    "    label.set_color('black')\n",
    "    label.set_text(label.get_text().replace('_', ' '))  # Remove underscores\n",
    "\n",
    "# Increase and bold x-axis numbers\n",
    "plt.xticks(fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Adding percentage labels\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.2f%%', label_type='edge', fontsize=14, color='black')  # Adjust font size and color\n",
    "\n",
    "# Enhance the legend\n",
    "plt.legend(fontsize=18, loc='lower right', frameon=True, framealpha=1, shadow=True, borderpad=1)\n",
    "legend = ax.legend()\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('black')\n",
    "frame.set_linewidth(2)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize(16)\n",
    "    text.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Techno-Corruption Category and Type Visualisation**\n",
    "Creates a horizontal bar chart using Matplotlib for Data Visualisation to show techno-corruption distribution across categories. It consolidates sub-type percentage contributions within each main category, uses custom colours for clarity, and annotates percentages and category sub-totals. Python plotting library matplotlib is chosen for it allows static, interactive, and animated visualisations. Useful for creating detailed charts and graphs to help interpret data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Define the updated groups and their percentages\n",
    "groups = {\n",
    "    \"Cybersecurity and Data Integrity\": {\n",
    "        \"Cyber Corruption\": 0.68,\n",
    "        \"Digital Identity Fraud\": 6.12,\n",
    "        \"Biometric Data Misuse\": 9.31,\n",
    "        \"Data Manipulation\": 6.85,\n",
    "        \"Digital Identity Theft for Corruption\": 5.18,\n",
    "        \"Cyber Espionage and Sabotage\": 3.96\n",
    "    },\n",
    "    \"Digital Financial Systems and Transactions\": {\n",
    "        \"Digital Ledger Technology (DLT) Corruption\": 3.13,\n",
    "        \"Cryptocurrency-Based Corruption\": 2.90,\n",
    "        \"Automated Corruption\": 3.44\n",
    "    },\n",
    "    \"Public Administration & e-Governance\": {\n",
    "        \"State Capture via Technology\": 1.99,\n",
    "        \"Corruption in Digital Public Infrastructure (DPI)\": 3.96,\n",
    "        \"Digital Bribery\": 5.21,\n",
    "        \"Electronic Procurement Fraud\": 6.27,\n",
    "        \"E-Governance Corrupt Appointments\": 1.41\n",
    "    },\n",
    "    \"Digital Elections Integrity and Political Systems\": {\n",
    "        \"Artificial Intelligence (AI) Corruption\": 3.24,\n",
    "        \"Electronic Voting System Manipulation\": 3.28,\n",
    "        \"Social Media Manipulation\": 4.97\n",
    "    },\n",
    "    \"Digital Contracts and Transactions\": {\n",
    "        \"Smart Contract Exploitation\": 2.90,\n",
    "        \"Blockchain System Vulnerabilities\": 5.08,\n",
    "        \"Encrypted Communications for Corruption\": 3.95\n",
    "    },\n",
    "    \"Technological Insider Threats and Institutional Integrity\": {\n",
    "        \"Insider Threats through Technology\": 5.31,\n",
    "        \"Budgeted Corruption via Technology\": 6.83,\n",
    "        \"Electronic Funds Transfer (EFT) Fraud\": 4.03\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert data to a pandas DataFrame\n",
    "data = []\n",
    "for category, sub_categories in groups.items():\n",
    "    for sub_category, value in sub_categories.items():\n",
    "        data.append([category, sub_category, value])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Category\", \"Sub-category\", \"Percentage\"])\n",
    "\n",
    "# Calculate sub-totals for each category\n",
    "sub_totals = df.groupby('Category')['Percentage'].sum().to_dict()\n",
    "\n",
    "# Correct sub-totals\n",
    "sub_totals[\"Digital Financial Systems and Transactions\"] = 3.13 + 2.90 + 3.44\n",
    "sub_totals[\"Cybersecurity and Data Integrity\"] = 0.68 + 6.12 + 9.31 + 6.85 + 5.18 + 3.96\n",
    "\n",
    "# Custom colorblind-friendly colors for each category\n",
    "colors = {\n",
    "    \"Cybersecurity and Data Integrity\": \"#0072B2\",\n",
    "    \"Digital Financial Systems and Transactions\": \"#D55E00\",\n",
    "    \"Public Administration & e-Governance\": \"#009E73\",\n",
    "    \"Digital Elections Integrity and Political Systems\": \"#CC79A7\",\n",
    "    \"Digital Contracts and Transactions\": \"#F0E442\",\n",
    "    \"Technological Insider Threats and Institutional Integrity\": \"#56B4E9\"\n",
    "}\n",
    "\n",
    "# Add sub-total percentage values to category labels in the legend box\n",
    "df['Color'] = df['Category'].map(colors)\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = df[\"Percentage\"].mean()\n",
    "\n",
    "# Create a horizontal bar chart\n",
    "plt.figure(figsize=(20, 12))  # Increased figure size\n",
    "bars = plt.barh(df[\"Sub-category\"], df[\"Percentage\"], color=df['Color'], height=0.6)  # Adjusted bar height\n",
    "\n",
    "# Add percentage tags at the edge of each sub-bar\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    plt.text(width + 0.2, bar.get_y() + bar.get_height() / 2, f'{width:.2f}%', ha='left', va='center', fontsize=14, fontweight='bold', color='black')\n",
    "\n",
    "# Add a vertical line for the average percentage\n",
    "plt.axvline(average_percentage, color='red', linestyle='--', linewidth=2, label=f'Average ({average_percentage:.2f}%)')\n",
    "\n",
    "# Add sub-total percentage values in the legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in colors]\n",
    "new_labels = [\n",
    "    f'Cybersecurity and Data Integrity (32.10%)',\n",
    "    f'Digital Financial Systems and Transactions (9.47%)',\n",
    "    f'Public Administration & e-Governance (18.84%)',\n",
    "    f'Digital Elections Integrity and Political Systems (11.49%)',\n",
    "    f'Digital Contracts and Transactions (11.93%)',\n",
    "    f'Technological Insider Threats and Institutional Integrity (16.17%)'\n",
    "]\n",
    "legend = plt.legend(handles, new_labels, title='Main Categories', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=16, title_fontsize=18, frameon=False)\n",
    "\n",
    "# Customize legend title\n",
    "plt.setp(legend.get_title(), fontsize=18, fontweight='bold', color='black')\n",
    "\n",
    "# Customize legend text color\n",
    "plt.setp(legend.get_texts(), color='black', fontweight='bold')\n",
    "\n",
    "# Set chart title and labels\n",
    "plt.title(\"Techno-Corruption Manifestations: Research-Based Types and Categories\", fontsize=30, weight='bold', loc='center', color='black')\n",
    "plt.xlabel(\"Percentage (%)\", fontsize=20, weight='bold', color='black')\n",
    "plt.ylabel(\"Types\", fontsize=20, weight='bold', color='black')\n",
    "\n",
    "# Make y-axis text labels bold and slightly larger\n",
    "plt.yticks(fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Make x-axis text labels bold and slightly larger\n",
    "plt.xticks(fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Add a text box for the average number legend\n",
    "plt.text(average_percentage + 0.2, -1, f'Average: {average_percentage:.2f}%', fontsize=16, fontweight='bold', color='red', bbox=dict(facecolor='white', edgecolor='red'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Visualising Cumulative Impact of Techno-Corruption Categories**\n",
    "This Matplotlib visualisation displays the cumulative effect of various categories of techno-corruption. The algorithm calculates the total contribution by adding up the percentages of each subtype within the main categories. The pie chart illustrates the relative significance of each category, providing clarity on their distribution and importance. Emphasising the comparative significance of different categories of techno-corruption clarifying which ones have the most profound influence. The utilisation of the \"donut\" style enhances legibility, while the implementation of category colours guarantees lucidity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the updated groups and their percentages\n",
    "groups = {\n",
    "    \"Cybersecurity and Data Integrity\": {\n",
    "        \"Cyber Corruption\": 0.68,\n",
    "        \"Digital Identity Fraud\": 6.12,\n",
    "        \"Biometric Data Misuse\": 9.31,\n",
    "        \"Data Manipulation\": 6.85,\n",
    "        \"Digital Identity Theft for Corruption\": 5.18,\n",
    "        \"Cyber Espionage and Sabotage\": 3.96\n",
    "    },\n",
    "    \"Digital Financial Systems and Transactions\": {\n",
    "        \"Digital Ledger Technology (DLT) Corruption\": 3.13,\n",
    "        \"Cryptocurrency-Based Corruption\": 2.90,\n",
    "        \"Automated Corruption\": 3.44\n",
    "    },\n",
    "    \"Public Administration & e-Governance\": {\n",
    "        \"State Capture via Technology\": 1.99,\n",
    "        \"Corruption in Digital Public Infrastructure (DPI)\": 3.96,\n",
    "        \"Digital Bribery\": 5.21,\n",
    "        \"Electronic Procurement Fraud\": 6.27,\n",
    "        \"E-Governance Corrupt Appointments\": 1.41\n",
    "    },\n",
    "    \"Digital Elections Integrity and Political Systems\": {\n",
    "        \"Artificial Intelligence (AI) Corruption\": 3.24,\n",
    "        \"Electronic Voting System Manipulation\": 3.28,\n",
    "        \"Social Media Manipulation\": 4.97\n",
    "    },\n",
    "    \"Digital Contracts and Transactions\": {\n",
    "        \"Smart Contract Exploitation\": 2.90,\n",
    "        \"Blockchain System Vulnerabilities\": 5.08,\n",
    "        \"Encrypted Communications for Corruption\": 3.95\n",
    "    },\n",
    "    \"Technological Insider Threats and Institutional Integrity\": {\n",
    "        \"Insider Threats through Technology\": 5.31,\n",
    "        \"Budgeted Corruption via Technology\": 6.83,\n",
    "        \"Electronic Funds Transfer (EFT) Fraud\": 4.03\n",
    "    }\n",
    "}\n",
    "\n",
    "# Correct cumulative weights for each main category\n",
    "techno_corruption_weight_tuples = [\n",
    "    (\"Cybersecurity and Data Integrity\", 0.68 + 6.12 + 9.31 + 6.85 + 5.18 + 3.96),\n",
    "    (\"Digital Financial Systems and Transactions\", 3.13 + 2.90 + 3.44),\n",
    "    (\"Public Administration & e-Governance\", 1.99 + 3.96 + 5.21 + 6.27 + 1.41),\n",
    "    (\"Digital Elections Integrity and Political Systems\", 3.24 + 3.28 + 4.97),\n",
    "    (\"Digital Contracts and Transactions\", 2.90 + 5.08 + 3.95),\n",
    "    (\"Technological Insider Threats and Institutional Integrity\", 5.31 + 6.83 + 4.03)\n",
    "]\n",
    "\n",
    "# Extract categories and their weights\n",
    "categories, weights = zip(*techno_corruption_weight_tuples)\n",
    "\n",
    "# Create a pie chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "colors = [\"#0072B2\", \"#D55E00\", \"#009E73\", \"#CC79A7\", \"#F0E442\", \"#56B4E9\"]\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(\n",
    "    weights,\n",
    "    labels=categories,\n",
    "    autopct='%1.1f%%',\n",
    "    startangle=140,\n",
    "    colors=colors,\n",
    "    wedgeprops={'edgecolor': 'black'},\n",
    "    textprops={'fontsize': 10, 'weight': 'bold', 'color': 'darkslategray'},\n",
    "    pctdistance=0.85\n",
    ")\n",
    "\n",
    "# Improve readability\n",
    "for text in texts + autotexts:\n",
    "    text.set_fontsize(12)  # Keep larger font size for readability\n",
    "    text.set_weight('bold')\n",
    "    text.set_color('black')  # Darken text color for better contrast\n",
    "\n",
    "# Draw circle for 'donut' style\n",
    "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
    "fig.gca().add_artist(centre_circle)\n",
    "\n",
    "# Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "ax.axis('equal')\n",
    "\n",
    "plt.title('Techno-Corruption Manifestations', fontsize=20, weight='bold', color='darkslategray')  # Enhance title legibility\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Exploring the Presence of Techno-Corruption Manifestations Observed during Auto-Ethnography**\n",
    "Creates a horizontal bar chart to illustrate the occurrence and prevalence of techno-corruption manifestations identified through auto-ethnography in a dataset of news articles. The tool combines and visualises the proportion of each manifestation category, improving the clarity and comprehensibility of the data using customised colours and annotations. The methodology entails conducting Exploratory Data Analysis (EDA), employing pandas to organise the data into a DataFrame, and utilising matplotlib and seaborn to generate a visual depiction. The code categorises the data based on the type of corruption, computes the proportions, and presents the outcomes in the form of a bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
    "\n",
    "# Data for the histogram\n",
    "auto_ethnography_corruption = {\n",
    "    \"Untraceable E-government Budget Implementation\": {\n",
    "        \"Budgeted Corruption via Technology\": 6.27,\n",
    "        \"Corruption in Digital Public Infrastructure (DPI)\": 3.96\n",
    "    },\n",
    "    \"Nonexistent Mobile Banking, Online, and Phone Payment Recipients\": {\n",
    "        \"Electronic Funds Transfer (EFT) Fraud\": 4.03,\n",
    "        \"Digital Identity Fraud\": 6.12\n",
    "    },\n",
    "    \"Fabricated Beneficiaries in Government Digital Databases\": {\n",
    "        \"Digital Identity Theft for Corruption\": 5.18,\n",
    "        \"Digital Identity Fraud\": 6.12\n",
    "    },\n",
    "    \"Biometric Verification of Ghost Recipients by Banks\": {\n",
    "        \"Biometric Data Misuse\": 9.31,\n",
    "        \"Digital Identity Fraud\": 6.12\n",
    "    },\n",
    "    \"Refusal to Fully Transition to Mobile Phone Cash Transfers\": {\n",
    "        \"Electronic Funds Transfer (EFT) Fraud\": 4.03,\n",
    "        \"Corruption in Digital Public Infrastructure (DPI)\": 3.96\n",
    "    },\n",
    "    \"Budget Shortfalls Filled with Social Protection Funds and Interest-Free Loans\": {\n",
    "        \"Budgeted Corruption via Technology\": 6.27,\n",
    "        \"Electronic Procurement Fraud\": 6.83\n",
    "    },\n",
    "    \"End-of-Budget Cycle Deficits in Social Protection Allocations\": {\n",
    "        \"Budgeted Corruption via Technology\": 6.27,\n",
    "        \"Electronic Procurement Fraud\": 6.83\n",
    "    },\n",
    "    \"Threats and Forced Disappearances of Whistleblowers\": {\n",
    "        \"Cyber Espionage and Sabotage\": 3.96,\n",
    "        \"Insider Threats through Technology\": 5.31\n",
    "    },\n",
    "    \"Government Ministers Shielded from Scrutiny and Accountability\": {\n",
    "        \"E-Governance Corrupt Appointments\": 1.41,\n",
    "        \"State Capture via Technology\": 1.99\n",
    "    }\n",
    "}\n",
    "\n",
    "# Convert data to a pandas DataFrame\n",
    "data = []\n",
    "for category, sub_categories in auto_ethnography_corruption.items():\n",
    "    for sub_category, value in sub_categories.items():\n",
    "        data.append([category, sub_category, value])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Category\", \"Sub-category\", \"Percentage\"])\n",
    "\n",
    "# Custom colorblind-friendly colors for each category\n",
    "colors = {\n",
    "    \"Untraceable E-government Budget Implementation\": \"#0072B2\",\n",
    "    \"Nonexistent Mobile Banking, Online, and Phone Payment Recipients\": \"#D55E00\",\n",
    "    \"Fabricated Beneficiaries in Government Digital Databases\": \"#009E73\",\n",
    "    \"Biometric Verification of Ghost Recipients by Banks\": \"#CC79A7\",\n",
    "    \"Refusal to Fully Transition to Mobile Phone Cash Transfers\": \"#F0E442\",\n",
    "    \"Budget Shortfalls Filled with Social Protection Funds and Interest-Free Loans\": \"#56B4E9\",\n",
    "    \"End-of-Budget Cycle Deficits in Social Protection Allocations\": \"#E69F00\",\n",
    "    \"Threats and Forced Disappearances of Whistleblowers\": \"#8E44AD\",\n",
    "    \"Government Ministers Shielded from Scrutiny and Accountability\": \"#C0392B\"\n",
    "}\n",
    "\n",
    "# Add color column to DataFrame\n",
    "df['Color'] = df['Category'].map(colors)\n",
    "\n",
    "# Simplify the y-axis labels\n",
    "df['Simple Category'] = df['Category']\n",
    "df_totals = df.groupby('Category')['Percentage'].sum().reset_index()\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = df_totals[\"Percentage\"].mean()\n",
    "\n",
    "# Create a figure\n",
    "fig, ax = plt.subplots(figsize=(24, 16))  # Reasonably increased the chart size\n",
    "\n",
    "# Bar chart\n",
    "bars = sns.barplot(x='Percentage', y='Category', data=df_totals, hue='Category', dodge=False, palette=colors, ax=ax, legend=False)\n",
    "bars.set_xlabel('Percentage (%)', fontsize=24, fontweight='bold')  # Increased font size\n",
    "bars.set_ylabel('Manifestations', fontsize=24, fontweight='bold')  # Increased font size\n",
    "bars.set_title('Techno-Corruption Manifestations Observed during Auto-Ethnography', fontsize=32, fontweight='bold', pad=40, color='black')  # Significantly increased and darkened title\n",
    "\n",
    "# Annotate bars with percentages\n",
    "for i, bar in enumerate(ax.patches):\n",
    "    y_text = bar.get_y() + bar.get_height() / 2\n",
    "    ax.text(bar.get_width() + 0.2, y_text, f'{df_totals.iloc[i][\"Percentage\"]:.2f}%',\n",
    "            color='black', ha=\"left\", va=\"center\", fontsize=20, fontweight='bold')  # Increased font size and bold\n",
    "\n",
    "# Increase the font size of the y-axis labels and make them darker\n",
    "bars.set_yticklabels(bars.get_yticklabels(), fontweight='bold', fontsize=22, color='black')  # Significantly increased font size and darkened\n",
    "\n",
    "# Use FixedLocator and FixedFormatter for y-axis labels\n",
    "ax.yaxis.set_major_locator(FixedLocator(range(len(df_totals))))\n",
    "ax.yaxis.set_major_formatter(FixedFormatter(df_totals['Category']))\n",
    "\n",
    "# Increase font size and darkness of x-axis labels\n",
    "ax.tick_params(axis='x', which='major', labelsize=20, labelcolor='black')  # Increased font size and darkened\n",
    "\n",
    "# Add a vertical line for the average percentage\n",
    "ax.axvline(average_percentage, color='red', linestyle='--', linewidth=2, label=f'Average ({average_percentage:.2f}%)')  # Increased line width\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Improved visualisation of above histogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "categories = [\n",
    "    'Biometric Verification of Ghost Recipients by Banks',\n",
    "    'Budget Shortfalls Filled with Social Protection Funds and Interest-Free Loans',\n",
    "    'End-of-Budget Cycle Deficits in Social Protection Allocations',\n",
    "    'Fabricated Beneficiaries in Government Digital Databases',\n",
    "    'Untraceable E-government Budget Implementation',\n",
    "    'Nonexistent Mobile Banking, Online, and Phone Payment Recipients',\n",
    "    'Threats and Forced Disappearances of Whistleblowers',\n",
    "    'Refusal to Fully Transition to Mobile Phone Cash Transfers',\n",
    "    'Government Ministers Shielded from Scrutiny and Accountability'\n",
    "]\n",
    "percentages = [15.43, 13.10, 13.10, 11.30, 10.23, 10.15, 9.27, 7.99, 3.40]\n",
    "average_percentage = 10.44\n",
    "\n",
    "# Sort the data\n",
    "sorted_indices = np.argsort(percentages)\n",
    "categories = [categories[i] for i in sorted_indices]\n",
    "percentages = [percentages[i] for i in sorted_indices]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "bars = ax.barh(categories, percentages, color=plt.cm.Blues(np.linspace(0.2, 0.8, len(categories))))\n",
    "\n",
    "# Add average line\n",
    "ax.axvline(average_percentage, color='red', linewidth=1.5, linestyle='--')\n",
    "ax.text(average_percentage + 0.3, -0.5, f'Average ({average_percentage}%)', color='red', fontsize=24, fontweight='bold')\n",
    "\n",
    "# Bar labels\n",
    "for bar, percentage in zip(bars, percentages):\n",
    "    ax.text(bar.get_width() + 0.3, bar.get_y() + bar.get_height() / 2, f'{percentage}%',\n",
    "            va='center', ha='left', fontsize=24, fontweight='bold', color='black')\n",
    "\n",
    "# Remove gaps between bars\n",
    "plt.subplots_adjust(left=0.3, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "# Increase label size and make them dark\n",
    "ax.set_xlabel('Percentage (%)', fontsize=28, fontweight='bold')\n",
    "ax.set_xticks(np.arange(0, max(percentages) + 5, 5))\n",
    "ax.set_xticklabels(np.arange(0, max(percentages) + 5, 5), fontsize=24, fontweight='bold', color='black')\n",
    "\n",
    "# Increase y-axis label size and make them bold\n",
    "ax.set_yticklabels(categories, fontsize=24, fontweight='bold')\n",
    "\n",
    "# Title\n",
    "ax.set_title('Techno-Corruption Manifestations Observed during Auto-Ethnography', fontsize=32, fontweight='bold')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparison of Techno-Corruption Manifestations in Auto-Ethnography and Research**\n",
    "Creates a detailed table comparing auto-ethnography and research-identified techno-corruption. It creates a matplotlib-styled table from a pandas DataFrame and formats and designs it to improve readability. Data is structured and visualised using pandas and matplotlib. Techno-corruption data is converted into a DataFrame and formatted into a table with alternate row colours, larger fonts, and clear categorisation. This method is part of Exploratory Data Analysis (EDA), which summarises and presents data for easy interpretation. EDA helps identify patterns, test hypotheses, and visualise data for better understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data for the table\n",
    "auto_ethnography_corruption = {\n",
    "    \"Untraceable E-government Budget Implementation\": {\n",
    "        \"Budgeted Corruption via Technology\": 6.27,\n",
    "        \"Corruption in Digital Public Infrastructure (DPI)\": 3.96\n",
    "    },\n",
    "    \"Nonexistent Mobile Banking, Online, and Phone Payment Recipients\": {\n",
    "        \"Electronic Funds Transfer (EFT) Fraud\": 4.03,\n",
    "        \"Digital Identity Fraud\": 6.12\n",
    "    },\n",
    "    \"Fabricated Beneficiaries in Government Digital Databases\": {\n",
    "        \"Digital Identity Theft for Corruption\": 5.18,\n",
    "        \"Digital Identity Fraud\": 6.12\n",
    "    },\n",
    "    \"Biometric Verification of Ghost Recipients by Banks\": {\n",
    "        \"Biometric Data Misuse\": 9.31,\n",
    "        \"Digital Identity Fraud\": 6.12\n",
    "    },\n",
    "    \"Refusal to Fully Transition to Mobile Phone Cash Transfers\": {\n",
    "        \"Electronic Funds Transfer (EFT) Fraud\": 4.03,\n",
    "        \"Corruption in Digital Public Infrastructure (DPI)\": 3.96\n",
    "    },\n",
    "    \"Budget Shortfalls Filled with Social Protection Funds and Interest-Free Loans\": {\n",
    "        \"Budgeted Corruption via Technology\": 6.27,\n",
    "        \"Electronic Procurement Fraud\": 6.83\n",
    "    },\n",
    "    \"End-of-Budget Cycle Deficits in Social Protection Allocations\": {\n",
    "        \"Budgeted Corruption via Technology\": 6.27,\n",
    "        \"Electronic Procurement Fraud\": 6.83\n",
    "    },\n",
    "    \"Threats and Forced Disappearances of Whistleblowers\": {\n",
    "        \"Cyber Espionage and Sabotage\": 3.96,\n",
    "        \"Insider Threats through Technology\": 5.31\n",
    "    },\n",
    "    \"Government Ministers Shielded from Scrutiny and Accountability\": {\n",
    "        \"E-Governance Corrupt Appointments\": 1.41,\n",
    "        \"State Capture via Technology\": 1.99\n",
    "    }\n",
    "}\n",
    "\n",
    "# Prepare the table data\n",
    "table_data = []\n",
    "seen = set()\n",
    "for category, sub_categories in auto_ethnography_corruption.items():\n",
    "    if category not in seen:\n",
    "        row = [category]\n",
    "        seen.add(category)\n",
    "        for sub_category, value in sub_categories.items():\n",
    "            row.append(f\"{sub_category} ({value}%)\")\n",
    "        if len(row) == 2:\n",
    "            row.append('')  # If only one associated type, add an empty cell\n",
    "        table_data.append(row)\n",
    "\n",
    "table_df = pd.DataFrame(table_data, columns=[\"Manifestation\", \"Associated Type 1\", \"Associated Type 2\"])\n",
    "\n",
    "# Create the table with enhanced design\n",
    "fig, ax = plt.subplots(figsize=(24, 14))  # Reasonably increased the chart size\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create the table\n",
    "table = ax.table(cellText=table_df.values, colLabels=table_df.columns, cellLoc='left', loc='center', colWidths=[0.3, 0.35, 0.35])\n",
    "\n",
    "# Add the topmost row for the overall title\n",
    "# Create a blank cell spanning all columns for the title\n",
    "title_cell = table.add_cell(-1, 0, width=1.05, height=0.05, loc='center', facecolor='#add8e6', edgecolor='black')\n",
    "title_cell.get_text().set_text(\"Auto-Ethnography Vs Research Based Techno-Corruption Types\")\n",
    "title_cell.get_text().set_fontsize(32)  # Increased font size\n",
    "title_cell.get_text().set_weight('bold')\n",
    "title_cell.get_text().set_color('black')  # Darkened color\n",
    "title_cell.get_text().set_ha('center')\n",
    "title_cell.get_text().set_va('center')\n",
    "\n",
    "# Create blank cells to span the title across all columns\n",
    "table.add_cell(-1, 1, width=0.0, height=0.05, loc='center', facecolor='#add8e6', edgecolor='black')\n",
    "table.add_cell(-1, 2, width=0.0, height=0.05, loc='center', facecolor='#add8e6', edgecolor='black')\n",
    "\n",
    "# Enhance table appearance with alternate row colors\n",
    "for (i, j), cell in table.get_celld().items():\n",
    "    cell.set_text_props(ha='left', fontsize=24, color='black')  # Increased font size and darker color\n",
    "    cell.set_height(0.05)  # Increase the height of each cell for better readability\n",
    "    if i == 0:\n",
    "        cell.set_text_props(weight='bold', ha='center', fontsize=28)  # Bold and larger header font\n",
    "        cell.set_facecolor('#b0c4de')\n",
    "    elif i % 2 == 0:\n",
    "        cell.set_facecolor('#e0e0e0')  # Enhanced background color shade\n",
    "    else:\n",
    "        cell.set_facecolor('#f0f0f0')  # Enhanced background color shade\n",
    "\n",
    "# Increase font size for better readability\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(24)\n",
    "\n",
    "# Enable text wrapping\n",
    "table.auto_set_column_width(col=list(range(len(table_df.columns))))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Technology Types that Enable and Combat Techno-Corruption in Kenya: Frequency Analysis**\n",
    "Preprocesses Kenyan news articles, counts the occurrences of predefined keywords related to technologies that enable or combat techno-corruption, and displays their frequency as a percentage using a horizontal bar chart. Starts with tokenization, stopword removal, and lemmatization, then regex-based keyword frequency counting and horizontal bar chart visualisation. The first method removes noise and standardize text to make it ready for analysis in text preprocessing. Then keyword frequency analysis looks for trends and insights in preprocessed text by counting terms or patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# # Import necessary libraries\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Load the dataset from an Excel file\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "df = pd.read_excel(excel_file_path, header=None,\n",
    "                   names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Define a preprocessing function to create processed text from 'Content'\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Advanced text preprocessing to remove punctuation, lowercase the text, lemmatize tokens, and filter out stopwords.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = tokenizer.tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.update(['per', 'cent', 'last', 'year'])  # Adding 'per', 'cent', 'last', 'year' to stop words\n",
    "    filtered_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Technology types used in Kenya and their keywords\n",
    "tech_keywords = {\n",
    "    \"E-Government Platforms\": [\n",
    "        \"digital government services\", \"e-procurement\", \"huduma centers\", \"cash transfer\",\n",
    "        \"transparency\", \"interoperability\", \"service delivery\", \"e-governance\", \"digital public infrastructure\", \"ICTs in governance\"\n",
    "    ],\n",
    "    \"National Digital Identity (Huduma Namba)\": [\n",
    "        \"digital identity\", \"citizen identification\", \"unique identifier\",\n",
    "        \"data breaches\", \"identity theft\", \"interoperability\", \"digital identity fraud\", \"identity management systems (IdMS)\"\n",
    "    ],\n",
    "    \"Integrated Financial Management Information System (IFMIS)\": [\n",
    "        \"government financial management\", \"budgeting\", \"expenditure management\", \"tenders\", \"contracts\", \"shell companies\",\n",
    "        \"financial reporting\", \"data manipulation\", \"system integrity\", \"financial transparency\", \"public financial management\"\n",
    "    ],\n",
    "    \"Smart Contracts\": [\n",
    "        \"automate contract execution\", \"supply chains\", \"blockchain\",\n",
    "        \"decentralized\", \"coding vulnerabilities\", \"manipulation\", \"smart contract exploitation\", \"digital ledger technology (DLT)\"\n",
    "    ],\n",
    "    \"Land Registry Systems\": [\n",
    "        \"land ownership\", \"land transfer\", \"secure records\", \"land registry\", \"ministry of lands\",\n",
    "        \"document forgery\", \"immutable records\", \"land registry\", \"digital land records\", \"land title management\", \"property registration\", \"land transaction transparency\"\n",
    "    ],\n",
    "    \"Electoral Systems\": [\n",
    "        \"electronic voting\", \"vote tampering\", \"electoral fraud\",\n",
    "        \"transparency\", \"public confidence\", \"electronic voting system manipulation\", \"vote verification\"\n",
    "    ],\n",
    "    \"Government Procurement Systems\": [\n",
    "        \"procurement processes\", \"e-procurement\", \"fraud prevention\", \"ghosts\", \"corruption\",\n",
    "        \"transparency\", \"document forgery\", \"public procurement\", \"digital procurement systems\"\n",
    "    ],\n",
    "    \"Supply Chain Management Systems\": [\n",
    "        \"transparency\", \"traceability\", \"counterfeit goods\",\n",
    "        \"blockchain\", \"efficiency\", \"supply chain transparency\", \"digital supply chains\"\n",
    "    ],\n",
    "    \"Financial Inclusion Technologies (e.g., M-PESA)\": [\n",
    "        \"mobile money\", \"financial services\", \"unbanked population\", \"social protection\", \"cash transfers\",\n",
    "        \"cryptocurrency\", \"financial inclusion\", \"mobile banking\", \"digital financial services\"\n",
    "    ],\n",
    "    \"Education Platforms (e.g., M-Shule)\": [\n",
    "        \"ai in education\", \"personalized learning\", \"student performance\", \"primary schools\", \"secondary schools\", \"university schools\",\n",
    "        \"data infrastructure\", \"e-learning\", \"digital education\", \"education\", \"sms-based learning\", \"adaptive learning systems\", \"mobile learning platforms\"\n",
    "    ],\n",
    "    \"Internet of Things (IoT)\": [\n",
    "        \"agriculture\", \"soil moisture monitoring\", \"weather conditions\",\n",
    "        \"remote patient monitoring\", \"efficiency\", \"smart agriculture\", \"iot healthcare\"\n",
    "    ],\n",
    "    \"Big Data and Analytics\": [\n",
    "        \"data analysis\", \"decision-making\", \"policy formulation\", \"civil service\", \"public service\",\n",
    "        \"fraud detection\", \"data privacy\", \"big data in governance\", \"predictive analytics\", \"data-driven insights\", \"large dataset analysis\", \"data mining\", \"real-time analytics\"\n",
    "    ],\n",
    "    \"USSD Technology\": [\n",
    "        \"mobile registration\", \"contribution processes\", \"social health insurance fund\", \"nhif\", \"shif\",\n",
    "        \"basic mobile phones\", \"ussd-based services\", \"mobile accessibility\", \"ussd codes\", \"low-cost mobile services\", \"text-based mobile services\"\n",
    "    ],\n",
    "    \"AI in Agriculture\": [\n",
    "        \"weather prediction\", \"water management\", \"food security\",\n",
    "        \"agricultural management\", \"smart farming\", \"ai-driven agriculture\"\n",
    "    ],\n",
    "    \"Digital Reporting and Whistleblowing Platforms\": [\n",
    "        \"anonymous reporting\", \"corruption\", \"extortion\",\n",
    "        \"whistleblower protection\", \"digital whistleblowing\", \"anti-corruption reporting\"\n",
    "    ],\n",
    "    \"Digital HR Management Systems\": [\n",
    "        \"merit-based recruitment\", \"promotion\", \"transparency\", \"manipulation\", \"hr digitalization\", \"employee management systems\"\n",
    "    ],\n",
    "    \"Financial Tracking Software\": [\n",
    "        \"financial transactions\", \"money laundering\", \"detection\", \"prevention\", \"financial monitoring\", \"anti-money laundering (AML) software\"\n",
    "    ],\n",
    "    \"Biometric Technology\": [\n",
    "        \"identity verification\", \"shif registration\", \"secure disbursement\", \"ghost workers\", \"biometric data security\",\n",
    "        \"biometric authentication\", \"ghosts\", \"non existent beneficiaries\", \"fraud\", \"fingerprint recognition\", \"facial recognition\", \"iris scans\", \"biometric data misuse\", \"privacy concerns\", \"biometric spoofing\"\n",
    "    ],\n",
    "    \"Management Information Systems (MIS)\": [\n",
    "        \"data management\", \"social protection\", \"beneficiary tracking\", \"mis in governance\", \"program management systems\", \"information systems integration\", \"data-driven decision making\", \"social welfare management\",\n",
    "        \"nhif\", \"beneficiaries registration\", \"field work officials\", \"cash transfers\"\n",
    "    ],\n",
    "    \"Electronic Payment Systems\": [\n",
    "        \"fund disbursement\", \"transparency\", \"efficiency\", \"bank accounts\", \"biometric authentication\", \"digital payments\", \"e-payment systems\"\n",
    "    ],\n",
    "    \"Blockchain Technology (Testing phase)\": [\n",
    "        \"immutable records\", \"land registry\", \"land transfer\", \"procurement\", \"financial transactions\", \"donor funds\", \"blockchain transparency\", \"DLT corruption\"\n",
    "    ],\n",
    "    \"Artificial Intelligence (AI) (Testing phase)\": [\n",
    "        \"anomaly detection\", \"personalized learning\", \"data analysis\", \"decision-making\", \"ai-driven decision-making\", \"land registry\", \"ethical ai\", \"machine learning\", \"title deeds\", \"land title\", \"land transfer\", \"ai governance\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Preprocess the Content column\n",
    "df['Processed_Content'] = df['Content'].dropna().apply(preprocess_text)\n",
    "\n",
    "# Function to count keyword occurrences using regex\n",
    "def count_keywords(text, keywords):\n",
    "    return sum(len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text)) for keyword in keywords)\n",
    "\n",
    "# Calculate the frequency of each terminology\n",
    "frequency = {term: 0 for term in tech_keywords}\n",
    "for _, row in df.iterrows():\n",
    "    processed_content = row['Processed_Content']\n",
    "    for term, keywords in tech_keywords.items():\n",
    "        frequency[term] += count_keywords(processed_content, keywords)\n",
    "\n",
    "# Calculate the percentage frequency\n",
    "total_mentions = sum(frequency.values())\n",
    "percentage_frequency = {term: (count / total_mentions) * 100 for term, count in frequency.items()} if total_mentions > 0 else {term: 0 for term in frequency}\n",
    "\n",
    "# Convert to DataFrame for visualization\n",
    "freq_df = pd.DataFrame(list(percentage_frequency.items()), columns=['Terminology', 'Percentage'])\n",
    "freq_df = freq_df.sort_values(by='Percentage', ascending=True)\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = freq_df['Percentage'].mean()\n",
    "\n",
    "# Enhanced Visualization with a Professional Blue Color Scheme using Seaborn\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x='Percentage', y='Terminology', data=freq_df, palette='Blues_d')\n",
    "ax.set_xlabel('Percentage (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Technology Types', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Technologies Enabling & Combating Techno-Corruption in Kenya', fontsize=20, fontweight='bold')\n",
    "\n",
    "# Adding average line\n",
    "plt.axvline(average_percentage, color='blue', linestyle='--', label=f'Average: {average_percentage:.2f}%')\n",
    "plt.legend()\n",
    "\n",
    "# Adding percentage labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f%%', label_type='edge', fontsize=10, fontweight='bold', color='black')\n",
    "\n",
    "# Making y-axis labels bold\n",
    "ax.set_yticklabels(ax.get_yticklabels(), fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Kenya's Digital and E-Government Corruption Types Identified by Government Task Force**\n",
    "Adopted from above for visualisation, in conjunction with the above histogram on technology types used in Kenya, to draw correlations between different technology use and emerging corruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚠️ Commented out to comply with offline environment\n",
    "# import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# Load dataset with error handling\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "try:\n",
    "    corruptnDF = pd.read_excel(excel_file_path, header=None,\n",
    "                               names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Government recognized \"corruption types\" in the present transformed digital and e-government era\n",
    "corruption_types = {\n",
    "    'Transaction Fraud': ['unauthorized transaction', 'hacking', 'financial systems', 'theft'],\n",
    "    'Public Corruption': ['bribery', 'embezzlement', 'nepotism'],\n",
    "    'Digital Fraud': ['digital fraud', 'digital lockers', 'personal data'],\n",
    "    'Service Delivery Corruption': ['bribery', 'favoritism', 'service delivery'],\n",
    "    'Land Registry Fraud': ['land registration', 'illegal land transfers', 'forgery', 'falsification of documents'],\n",
    "    'Supply Chain Fraud': ['supply chain', 'counterfeit goods', 'document forgery'],\n",
    "    'Bribery': ['bribery', 'offering value', 'receiving value'],\n",
    "    'Embezzlement': ['embezzlement', 'misappropriation', 'diverted funds'],\n",
    "    'Nepotism': ['nepotism', 'favoritism', 'relatives'],\n",
    "    'Tax Evasion': ['tax evasion', 'underpayment of tax', 'non-payment of tax'],\n",
    "    'Counterfeit Goods': ['counterfeit goods', 'unauthorized manufacturing', 'distribution'],\n",
    "    'Vote Tampering': ['vote tampering', 'election interference', 'vote tallying'],\n",
    "    'Misuse Public Office': ['abuse', 'favoring contractors', 'personal gain', 'nys'],\n",
    "    'Fraudulent Documentation': ['forgery', 'false documents', 'deception', 'fake'],\n",
    "    'Money Laundering': ['money laundering', 'criminal activity', 'legal funds'],\n",
    "    'Procurement Fraud': ['ghost', 'procurement fraud', 'false claims', 'procurement contract', 'ifmis'],\n",
    "    'Cyber Fraud': ['cyber fraud', 'internet services', 'software']\n",
    "}\n",
    "\n",
    "# Initialize necessary components for text preprocessing\n",
    "# ⚠️ Commented out: nltk.download('punkt', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('stopwords', quiet=True)\n",
    "# ⚠️ Commented out: nltk.download('wordnet', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Preprocess text data by converting to lower case, removing punctuation, tokenizing, removing stopwords, and lemmatizing.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(text) if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing to the Content column\n",
    "corruptnDF['Processed_Content'] = corruptnDF['Content'].dropna().apply(preprocess)\n",
    "\n",
    "def count_keywords(text, keywords):\n",
    "    \"\"\"Count keyword occurrences in text using regex.\"\"\"\n",
    "    return sum(len(re.findall(r'\\b' + re.escape(keyword.lower()) + r'\\b', text)) for keyword in keywords)\n",
    "\n",
    "# Initialize frequency dictionary\n",
    "frequency = {term: 0 for term in corruption_types}\n",
    "\n",
    "# Calculate the frequency of each corruption type\n",
    "for _, row in corruptnDF.iterrows():\n",
    "    processed_content = row['Processed_Content']\n",
    "    for term, keywords in corruption_types.items():\n",
    "        frequency[term] += count_keywords(processed_content, keywords)\n",
    "\n",
    "# Calculate the percentage frequency\n",
    "total_mentions = sum(frequency.values())\n",
    "percentage_frequency = {term: (count / total_mentions) * 100 if total_mentions > 0 else 0 for term, count in frequency.items()}\n",
    "\n",
    "# Convert to DataFrame for visualization and sort in descending order\n",
    "freq_df = pd.DataFrame(list(percentage_frequency.items()), columns=['Corruption Type', 'Percentage'])\n",
    "freq_df = freq_df.sort_values(by='Percentage', ascending=False)\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = sum(freq_df['Percentage']) / len(freq_df)\n",
    "\n",
    "# Enhanced Visualization with a Green Color Scheme and Average Line\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.barplot(x='Percentage', y='Corruption Type', data=freq_df, palette='Greens_d')\n",
    "ax.set_xlabel('Percentage (%)', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Corruption Types', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_title('Corruption Types in Digital and E-Government As Per Kenya TaskForce', fontsize=28, fontweight='bold', color='black', pad=20)\n",
    "\n",
    "# Add average line\n",
    "plt.axvline(x=average_percentage, color='darkgreen', linestyle='--', label=f'Average: {average_percentage:.2f}%', linewidth=3)\n",
    "\n",
    "# Enhance y-axis labels\n",
    "for label in ax.get_yticklabels():\n",
    "    label.set_fontsize(16)\n",
    "    label.set_fontweight('bold')\n",
    "    label.set_color('black')\n",
    "\n",
    "# Increase and bold x-axis numbers\n",
    "plt.xticks(fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Adding percentage labels\n",
    "for i in ax.containers:\n",
    "    ax.bar_label(i, fmt='%.2f%%', label_type='edge', fontsize=14, color='black')  # Adjust font size and color\n",
    "\n",
    "# Enhance the legend\n",
    "plt.legend(fontsize=18, loc='lower right', frameon=True, framealpha=1, shadow=True, borderpad=1)\n",
    "legend = ax.legend()\n",
    "frame = legend.get_frame()\n",
    "frame.set_facecolor('white')\n",
    "frame.set_edgecolor('black')\n",
    "frame.set_linewidth(2)\n",
    "for text in legend.get_texts():\n",
    "    text.set_fontsize(14)\n",
    "    text.set_color('black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Visual Representation and Analysis of Technology Applications in Combating and Enabling Techno-Corruption in Kenya**\n",
    "Executed in two parts, part one generates a horizontal bar chart to show the frequency of technologies used to combat and enable techno-corruption in Kenya, and part two generates a detailed table listing these technologies and their uses. Data Visualisation and Tabular Representation methodologies are applied in sorting data by technology usage percentage, creating a bar chart to show their prevalence, and creating a detailed table to list their diverse areas of applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Data extracted from the histogram\n",
    "data = {\n",
    "    \"Technology\": [\n",
    "        \"E-Government Platforms\", \"Government Procurement Systems\", \"Digital Reporting and Whistleblowing Platforms\",\n",
    "        \"Supply Chain Management Systems\", \"Management Information Systems (MIS)\", \"Financial Inclusion Technologies\",\n",
    "        \"Smart Contracts\", \"Biometric Technology\", \"Electronic Payment Systems\", \"Education Platforms (e.g., M-Shule)\",\n",
    "        \"Digital HR Management Systems\", \"Blockchain Technology (Testing phase)\", \"Electoral Systems\", \"USSD Technology\",\n",
    "        \"Internet of Things (IoT)\", \"Big Data and Analytics\", \"Financial Tracking Software\", \"AI in Agriculture\",\n",
    "        \"Integrated Financial Management Information System (IFMIS)\", \"National Digital Identity (Huduma Namba)\",\n",
    "        \"Land Registry Systems\", \"Artificial Intelligence (AI) (Testing phase)\"\n",
    "    ],\n",
    "    \"Percentage\": [\n",
    "        10.77, 19.10, 16.76, 8.79, 7.01, 6.28, 5.80, 3.77, 3.12, 2.79, 2.40, 2.33, 2.26, 1.97, 1.84, 1.81, 1.33, 0.60,\n",
    "        0.44, 0.34, 0.29, 0.22\n",
    "    ],\n",
    "    \"Applications\": [\n",
    "        \"E-Procurement & Huduma Centers\", \"Streamlined Procurement\", \"Anonymous Reporting\", \"Transparent Supply Chains\",\n",
    "        \"Data Management\", \"Mobile Money & Cryptocurrency\", \"Automated Contracts\", \"Identity Verification\",\n",
    "        \"Secure Payments\", \"Personalized Learning\", \"Merit-Based HR\", \"Immutable Records\", \"Transparent Elections\",\n",
    "        \"Social Protection & SHIF Health Insurance\", \"Agriculture & Healthcare\", \"Public Services & Financial Sector\",\n",
    "        \"Money Laundering Detection\", \"Food Security\", \"Financial Management\", \"Citizen Identification\", \"Secure Land Records\",\n",
    "        \"Anomaly Detection, Personalized Learning & Data Analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort dataframe by Percentage in descending order\n",
    "df = df.sort_values(by='Percentage', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = df['Percentage'].mean()\n",
    "\n",
    "# Create a figure with the bar chart only\n",
    "fig, ax0 = plt.subplots(figsize=(20, 12))\n",
    "sns.barplot(x='Percentage', y='Technology', data=df, palette='Blues_d', ax=ax0)\n",
    "ax0.set_xlabel('Percentage (%)', fontsize=20, fontweight='bold', color='black')\n",
    "ax0.set_ylabel('Technology Types', fontsize=18, fontweight='bold', color='black')\n",
    "ax0.set_title('Applications of Different Technologies in Kenya', fontsize=24, fontweight='bold', color='black')\n",
    "\n",
    "# Adding average line\n",
    "plt.axvline(average_percentage, color='darkblue', linestyle='--', label=f'Average: {average_percentage:.2f}%')\n",
    "plt.legend(fontsize=16, title_fontsize='13')\n",
    "\n",
    "# Making y-axis labels bold and larger\n",
    "ax0.set_yticklabels(ax0.get_yticklabels(), fontweight='bold', fontsize=14, color='black')\n",
    "\n",
    "# Making x-axis labels larger and darker\n",
    "ax0.set_xticklabels(ax0.get_xticks(), fontsize=14, fontweight='bold', color='black')\n",
    "\n",
    "# Adding percentage labels at the end of each bar\n",
    "for index, row in df.iterrows():\n",
    "    ax0.text(row.Percentage + 0.2, index, f'{row.Percentage}%', color='black', ha=\"left\", va=\"center\", fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysing Applications for Above Technologies in Kenya**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data extracted from the histogram\n",
    "data = {\n",
    "    \"Technology\": [\n",
    "        \"E-Government Platforms\", \"Government Procurement Systems\", \"Digital Reporting and Whistleblowing Platforms\",\n",
    "        \"Supply Chain Management Systems\", \"Management Information Systems (MIS)\", \"Financial Inclusion Technologies\",\n",
    "        \"Smart Contracts\", \"Biometric Technology\", \"Electronic Payment Systems\", \"Education Platforms (e.g., M-Shule)\",\n",
    "        \"Digital HR Management Systems\", \"Blockchain Technology (Testing phase)\", \"Electoral Systems\", \"USSD Technology\",\n",
    "        \"Internet of Things (IoT)\", \"Big Data and Analytics\", \"Financial Tracking Software\", \"AI in Agriculture\",\n",
    "        \"Integrated Financial Management Information System (IFMIS)\", \"National Digital Identity (Huduma Namba)\",\n",
    "        \"Land Registry Systems\", \"Artificial Intelligence (AI) (Testing phase)\"\n",
    "    ],\n",
    "    \"Percentage\": [\n",
    "        10.77, 19.10, 16.76, 8.79, 7.01, 6.28, 5.80, 3.77, 3.12, 2.79, 2.40, 2.33, 2.26, 1.97, 1.84, 1.81, 1.33, 0.60,\n",
    "        0.44, 0.34, 0.29, 0.22\n",
    "    ],\n",
    "    \"Applications\": [\n",
    "        \"E-Procurement & Huduma Centers\", \"Streamlined Procurement\", \"Anonymous Reporting\", \"Transparent Supply Chains\",\n",
    "        \"Data Management\", \"Mobile Money & Cryptocurrency\", \"Automated Contracts\", \"Identity Verification\",\n",
    "        \"Secure Payments\", \"Personalized Learning\", \"Merit-Based HR\", \"Immutable Records\", \"Transparent Elections\",\n",
    "        \"Social Protection & SHIF Health Insurance\", \"Agriculture & Healthcare\", \"Public Services & Financial Sector\",\n",
    "        \"Money Laundering Detection\", \"Food Security\", \"Financial Management\", \"Citizen Identification\", \"Secure Land Records\",\n",
    "        \"Anomaly Detection, Personalized Learning & Data Analysis\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sort dataframe by Percentage in descending order\n",
    "df = df.sort_values(by='Percentage', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Create a separate figure for the table\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "ax1.axis('tight')\n",
    "ax1.axis('off')\n",
    "table_data = df[['Technology', 'Applications']]\n",
    "table = ax1.table(cellText=table_data.values, colLabels=table_data.columns, cellLoc='left', loc='center', colColours=['#e0e0e0', '#e0e0e0'])\n",
    "\n",
    "# Enhance table appearance with alternate row colors and deep dark text color\n",
    "for (i, j), cell in table.get_celld().items():\n",
    "    if i == 0:\n",
    "        cell.set_text_props(weight='bold', ha='center', fontsize=24, color='black')  # Increased font size for header\n",
    "        cell.set_facecolor('#b0c4de')\n",
    "    elif i % 2 == 0:\n",
    "        cell.set_facecolor('#f0f0f0')\n",
    "    else:\n",
    "        cell.set_facecolor('#ffffff')\n",
    "    cell.set_text_props(ha='left', fontsize=22, color='black')  # Deep dark color for content\n",
    "    cell.set_edgecolor('#d0d0d0')\n",
    "\n",
    "# Adjust row height\n",
    "table.scale(1, 1.3)  # Slightly increased row height\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Key Actors in Techno-Corruption: Entity Network and Community Detection**\n",
    "Uses a large corpus of news articles to build and visualise a techno-corruption network of organisations and individuals. Extracting entities and relationships from text reveals dataset connections and communities. Using spaCy and NLP to identify named entities in text data. Also utilised is NetworkX as it is a Python package for creating, manipulating, and studying complex network structure, dynamics, and functions. Then uses a community detection algorithm known as Louvain method that maximises modularity to divide the network into communities. Finally using centrality measures to identify the most important network nodes based on their connections. Visualising a network graph of key actors and their connections that reveals techno-corruption structures and communities. Helping explain their interactions and potential influence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from community import community_louvain\n",
    "\n",
    "# Load Spacy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Increase the max_length limit\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# Load the dataset\n",
    "Corpus = \"318NewsDataSet.xlsx\"\n",
    "CorruptnDF = pd.read_excel(Corpus, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Use the 'Content' column as the corpus\n",
    "corpus_content = \" \".join(CorruptnDF['Content'].dropna().tolist())\n",
    "\n",
    "# Function to normalize entity names\n",
    "def normalize_entity(entity):\n",
    "    entity = entity.strip()\n",
    "    replacements = {\n",
    "        'the world bank': 'World Bank',\n",
    "        'world bank': 'World Bank',\n",
    "        'the eu': 'EU',\n",
    "        'eu': 'EU',\n",
    "        'the un': 'UN',\n",
    "        'un': 'UN'\n",
    "    }\n",
    "    return replacements.get(entity.lower(), entity)\n",
    "\n",
    "# Process the corpus with spaCy in chunks\n",
    "def process_text_in_chunks(text, chunk_size=500000):  # Adjusted chunk size for better memory management\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        yield nlp(text[i:i + chunk_size])\n",
    "\n",
    "# Initialize a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Error handling in the main processing loop\n",
    "try:\n",
    "    # Iterate through the text chunks and process entities\n",
    "    for doc in process_text_in_chunks(corpus_content):\n",
    "        for sentence in doc.sents:\n",
    "            entities = [normalize_entity(ent.text) for ent in sentence.ents if ent.label_ in ['PERSON', 'ORG']]\n",
    "            for ent in entities:\n",
    "                for ent2 in entities:\n",
    "                    if ent != ent2:  # Avoid self-loops\n",
    "                        G.add_edge(ent, ent2)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during text processing: {e}\")\n",
    "\n",
    "# Apply community detection with error handling\n",
    "try:\n",
    "    partition = community_louvain.best_partition(G)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during community detection: {e}\")\n",
    "    partition = {}\n",
    "\n",
    "# Calculate centrality measures with error handling\n",
    "try:\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during centrality calculation: {e}\")\n",
    "    degree_centrality = {}\n",
    "\n",
    "# Select top nodes by degree centrality\n",
    "top_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:50]\n",
    "H = G.subgraph(top_nodes).copy()\n",
    "\n",
    "# Draw the network with error handling\n",
    "try:\n",
    "    plt.figure(figsize=(24, 24))  # Increase figure size for better readability\n",
    "    pos = nx.spring_layout(H, k=0.9)  # Adjust layout for better spacing\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.6, edge_color='black')  # Darker edge color\n",
    "    node_color = [partition[node] if node in partition else 0 for node in H]\n",
    "    node_size = [H.degree(node) * 100 for node in H]  # Increase node size for better readability\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_color, node_size=node_size, cmap=plt.cm.Blues, alpha=0.8)\n",
    "    nx.draw_networkx_labels(H, pos, font_size=16, font_weight='bold', verticalalignment='center', font_color='black')  # Increase font size and use darker font color\n",
    "\n",
    "    plt.title(\"Network of Key Actors in Techno-Corruption Areas\", fontsize=30, fontweight='bold', color='black')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during graph drawing: {e}\")\n",
    "\n",
    "# Print basic network statistics for the new graph with error handling\n",
    "try:\n",
    "    print(f\"Graph - Number of nodes: {H.number_of_nodes()}\")\n",
    "    print(f\"Graph - Number of edges: {H.number_of_edges()}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while printing graph statistics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Centrality Measures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from community import community_louvain\n",
    "\n",
    "# Load Spacy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Increase the max_length limit\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# Load the dataset\n",
    "Corpus = \"318NewsDataSet.xlsx\"\n",
    "CorruptnDF = pd.read_excel(Corpus, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Use the 'Content' column as the corpus\n",
    "corpus_content = \" \".join(CorruptnDF['Content'].dropna().tolist())\n",
    "\n",
    "# Function to normalize entity names\n",
    "def normalize_entity(entity):\n",
    "    entity = entity.strip()\n",
    "    replacements = {\n",
    "        'the world bank': 'World Bank',\n",
    "        'world bank': 'World Bank',\n",
    "        'the eu': 'EU',\n",
    "        'eu': 'EU',\n",
    "        'the un': 'UN',\n",
    "        'un': 'UN'\n",
    "    }\n",
    "    return replacements.get(entity.lower(), entity)\n",
    "\n",
    "# Process the corpus with spaCy in chunks\n",
    "def process_text_in_chunks(text, chunk_size=500000):  # Adjusted chunk size for better memory management\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        yield nlp(text[i:i + chunk_size])\n",
    "\n",
    "# Initialize a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Error handling in the main processing loop\n",
    "try:\n",
    "    # Iterate through the text chunks and process entities\n",
    "    for doc in process_text_in_chunks(corpus_content):\n",
    "        for sentence in doc.sents:\n",
    "            entities = [normalize_entity(ent.text) for ent in sentence.ents if ent.label_ in ['PERSON', 'ORG']]\n",
    "            for ent in entities:\n",
    "                for ent2 in entities:\n",
    "                    if ent != ent2:  # Avoid self-loops\n",
    "                        G.add_edge(ent, ent2)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during text processing: {e}\")\n",
    "\n",
    "# Apply community detection with error handling\n",
    "try:\n",
    "    partition = community_louvain.best_partition(G)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during community detection: {e}\")\n",
    "    partition = {}\n",
    "\n",
    "# Calculate centrality measures with error handling\n",
    "try:\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during centrality calculation: {e}\")\n",
    "    degree_centrality = {}\n",
    "\n",
    "# Select top nodes by degree centrality\n",
    "top_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:50]\n",
    "H = G.subgraph(top_nodes).copy()\n",
    "\n",
    "# Draw the network with error handling\n",
    "try:\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    pos = nx.spring_layout(H, k=0.9)  # Adjust layout for better spacing\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.4)\n",
    "    node_color = [partition[node] if node in partition else 0 for node in H]\n",
    "    node_size = [H.degree(node) * 20 for node in H]  # Adjust node size for better readability\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_color, node_size=node_size, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    nx.draw_networkx_labels(H, pos, font_size=14, font_weight='bold', verticalalignment='center')\n",
    "\n",
    "    plt.title(\"Network of Key Actors in the Sector\", fontsize=20, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during graph drawing: {e}\")\n",
    "\n",
    "# Print basic network statistics for the new graph with error handling\n",
    "try:\n",
    "    print(f\"Graph - Number of nodes: {H.number_of_nodes()}\")\n",
    "    print(f\"Graph - Number of edges: {H.number_of_edges()}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while printing graph statistics: {e}\")\n",
    "\n",
    "# Calculate additional centrality measures\n",
    "try:\n",
    "    betweenness_centrality = nx.betweenness_centrality(H)\n",
    "    initial_guess = {node: 1.0 for node in H}\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(H, max_iter=1000, tol=1e-06, nstart=initial_guess)\n",
    "\n",
    "    # Print top 10 nodes by degree centrality\n",
    "    print(\"\\nTop 10 nodes by degree centrality:\")\n",
    "    for node, centrality in sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{node}: {centrality}\")\n",
    "\n",
    "    # Print top 10 nodes by betweenness centrality\n",
    "    print(\"\\nTop 10 nodes by betweenness centrality:\")\n",
    "    for node, centrality in sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{node}: {centrality}\")\n",
    "\n",
    "    # Print top 10 nodes by eigenvector centrality\n",
    "    print(\"\\nTop 10 nodes by eigenvector centrality:\")\n",
    "    for node, centrality in sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{node}: {centrality}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during additional centrality calculations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Focusing on Other Nodes in the Network - Like EU and UN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from community import community_louvain\n",
    "\n",
    "# Load Spacy's small English model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Increase the max_length limit\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# Load the dataset\n",
    "Corpus = \"318NewsDataSet.xlsx\"\n",
    "CorruptnDF = pd.read_excel(Corpus, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Use the 'Content' column as the corpus\n",
    "corpus_content = \" \".join(CorruptnDF['Content'].dropna().tolist())\n",
    "\n",
    "# Function to normalize entity names\n",
    "def normalize_entity(entity):\n",
    "    entity = entity.strip()\n",
    "    replacements = {\n",
    "        'the world bank': 'World Bank',\n",
    "        'world bank': 'World Bank',\n",
    "        'the eu': 'EU',\n",
    "        'eu': 'EU',\n",
    "        'the un': 'UN',\n",
    "        'un': 'UN'\n",
    "    }\n",
    "    return replacements.get(entity.lower(), entity)\n",
    "\n",
    "# Process the corpus with spaCy in chunks\n",
    "def process_text_in_chunks(text, chunk_size=500000):  # Adjusted chunk size for better memory management\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        yield nlp(text[i:i + chunk_size])\n",
    "\n",
    "# Initialize a network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Error handling in the main processing loop\n",
    "try:\n",
    "    # Iterate through the text chunks and process entities\n",
    "    for doc in process_text_in_chunks(corpus_content):\n",
    "        for sentence in doc.sents:\n",
    "            entities = [normalize_entity(ent.text) for ent in sentence.ents if ent.label_ in ['PERSON', 'ORG']]\n",
    "            for ent in entities:\n",
    "                for ent2 in entities:\n",
    "                    if ent != ent2:  # Avoid self-loops\n",
    "                        G.add_edge(ent, ent2)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during text processing: {e}\")\n",
    "\n",
    "# Apply community detection with error handling\n",
    "try:\n",
    "    partition = community_louvain.best_partition(G)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during community detection: {e}\")\n",
    "    partition = {}\n",
    "\n",
    "# Calculate centrality measures with error handling\n",
    "try:\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "    initial_guess = {node: 1.0 for node in G}\n",
    "    eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, tol=1e-06, nstart=initial_guess)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during centrality calculation: {e}\")\n",
    "    degree_centrality, betweenness_centrality, eigenvector_centrality = {}, {}, {}\n",
    "\n",
    "# Print centrality values for all nodes to ensure UN and EU are included\n",
    "print(\"\\nCentrality values for all nodes:\")\n",
    "for node in G.nodes():\n",
    "    print(f\"{node}: Degree = {degree_centrality.get(node, 0)}, Betweenness = {betweenness_centrality.get(node, 0)}, Eigenvector = {eigenvector_centrality.get(node, 0)}\")\n",
    "\n",
    "# Ensure that UN and EU are included in the results\n",
    "for node in ['UN', 'EU']:\n",
    "    if node not in degree_centrality:\n",
    "        degree_centrality[node] = 0\n",
    "    if node not in betweenness_centrality:\n",
    "        betweenness_centrality[node] = 0\n",
    "    if node not in eigenvector_centrality:\n",
    "        eigenvector_centrality[node] = 0\n",
    "\n",
    "# Select top nodes by degree centrality\n",
    "top_nodes = sorted(degree_centrality, key=degree_centrality.get, reverse=True)[:50]\n",
    "H = G.subgraph(top_nodes).copy()\n",
    "\n",
    "# Draw the network with error handling\n",
    "try:\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    pos = nx.spring_layout(H, k=0.9)  # Adjust layout for better spacing\n",
    "    nx.draw_networkx_edges(H, pos, alpha=0.4)\n",
    "    node_color = [partition[node] if node in partition else 0 for node in H]\n",
    "    node_size = [H.degree(node) * 20 for node in H]  # Adjust node size for better readability\n",
    "    nx.draw_networkx_nodes(H, pos, node_color=node_color, node_size=node_size, cmap=plt.cm.Blues, alpha=0.5)\n",
    "    nx.draw_networkx_labels(H, pos, font_size=14, font_weight='bold', verticalalignment='center')\n",
    "\n",
    "    plt.title(\"Network of Key Actors in the Sector\", fontsize=20, fontweight='bold')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during graph drawing: {e}\")\n",
    "\n",
    "# Print basic network statistics for the new graph with error handling\n",
    "try:\n",
    "    print(f\"Graph - Number of nodes: {H.number_of_nodes()}\")\n",
    "    print(f\"Graph - Number of edges: {H.number_of_edges()}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while printing graph statistics: {e}\")\n",
    "\n",
    "# Print top 10 nodes by degree centrality\n",
    "try:\n",
    "    print(\"\\nTop 10 nodes by degree centrality:\")\n",
    "    for node, centrality in sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{node}: {centrality}\")\n",
    "\n",
    "    # Print top 10 nodes by betweenness centrality\n",
    "    print(\"\\nTop 10 nodes by betweenness centrality:\")\n",
    "    for node, centrality in sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{node}: {centrality}\")\n",
    "\n",
    "    # Print top 10 nodes by eigenvector centrality\n",
    "    print(\"\\nTop 10 nodes by eigenvector centrality:\")\n",
    "    for node, centrality in sorted(eigenvector_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"{node}: {centrality}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during additional centrality calculations: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Examining the Frequency of Anti-Corruption Measures in News Data**\n",
    "Conducts analyses and generates visualisations to showcase the frequency of research based anti-corruption measures mentioned in news articles. Retrieving data from an Excel dataset, standardising and classifying anti-corruption measures, computing their occurrence and proportion, and presenting the outcomes in a vertical bar chart. Utilising Natural Language Processing (NLP) methods to analyse text data for references to pre-established categories related to anti-corruption, as indicated by research. Utilising regular expressions to precisely associate the specific terms with more general categories and calculates the frequency of their appearances throughout the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Load the dataset\n",
    "excel_file_path = '318NewsDataSet.xlsx'  # Ensure this path is correct\n",
    "CorruptnDF = pd.read_excel(excel_file_path, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Define category mappings for specific terms to broader categories using regex for precision\n",
    "category_mapping = {\n",
    "    r\"\\basset recovery\\b\": \"Asset Recovery\",\n",
    "    r\"\\bpublic sector\\b\": \"Public Sector Transparency\",\n",
    "    r\"\\bprivate sector\\b\": \"Private Sector Accountability\",\n",
    "    r\"\\bwhistleblower protection\\b\": \"Whistleblower Protection\",\n",
    "    r\"\\bwitness protection\\b\": \"Witness Protection\",\n",
    "    r\"\\banti-money laundering\\b\": \"Anti-Money Laundering\",\n",
    "    r\"\\bfinancial transparency\\b\": \"Financial Transparency\",\n",
    "    r\"\\baudits?\\b\": \"Audit\",\n",
    "    r\"\\bcompliance\\b\": \"Compliance\",\n",
    "    r\"\\bethics\\b\": \"Ethics\",\n",
    "    r\"\\bintegrity\\b\": \"Integrity\",\n",
    "    r\"\\boversight\\b\": \"Oversight\",\n",
    "    r\"\\bregulation\\b\": \"Regulation\",\n",
    "    r\"\\breform\\b\": \"Reform\",\n",
    "    r\"\\bcode of conduct\\b\": \"Code of Conduct\",\n",
    "    r\"\\bpublic procurement\\b\": \"Public Procurement\",\n",
    "    r\"\\bdeclaration of assets\\b\": \"Declaration of Assets\",\n",
    "    r\"\\bcapacity building\\b\": \"Capacity Building\",\n",
    "    r\"\\bpublic awareness\\b\": \"Public Awareness\",\n",
    "    r\"\\beducation programs\\b\": \"Education Programs\"\n",
    "}\n",
    "\n",
    "# Initialize category counts directly\n",
    "category_counts = Counter({value: 0 for value in category_mapping.values()})\n",
    "\n",
    "# Normalize categories and count occurrences\n",
    "def normalize_and_count_measures(text):\n",
    "    text_lower = text.lower()  # Convert to lowercase once\n",
    "    for pattern, category in category_mapping.items():\n",
    "        if re.search(pattern, text_lower):  # Improved accuracy with regex\n",
    "            category_counts[category] += 1\n",
    "\n",
    "# Apply the function across the 'Content' column\n",
    "CorruptnDF['Content'].dropna().apply(normalize_and_count_measures)\n",
    "\n",
    "# Calculate percentages\n",
    "total_mentions = sum(category_counts.values())\n",
    "category_percentages = {cat: (count / total_mentions) * 100 for cat, count in category_counts.items() if count > 0}\n",
    "\n",
    "# Sort the percentages in descending order\n",
    "sorted_categories = sorted(category_percentages, key=category_percentages.get, reverse=True)\n",
    "sorted_percentages = [category_percentages[cat] for cat in sorted_categories]\n",
    "\n",
    "# Calculate the average percentage\n",
    "average_percentage = sum(sorted_percentages) / len(sorted_percentages)\n",
    "\n",
    "# Visualization with vertical bar chart\n",
    "plt.figure(figsize=(20, 12))\n",
    "bars = plt.bar(sorted_categories, sorted_percentages, color=plt.cm.tab20c.colors[:len(sorted_percentages)])\n",
    "\n",
    "# Adding percentage labels at the tip of each bar\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, height, f'{height:.1f}%', va='bottom', ha='center', fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.xlabel('Anti-Corruption Measures', fontsize=24, fontweight='bold')\n",
    "plt.ylabel('Percentage (%)', fontsize=24, fontweight='bold')\n",
    "plt.title('Frequency of Anti-Corruption Measures in News Data', fontsize=28, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right', fontsize=20, fontweight='bold')\n",
    "plt.yticks(fontsize=20, fontweight='bold')\n",
    "\n",
    "# Add average line\n",
    "plt.axhline(average_percentage, color='red', linestyle='--', linewidth=2)\n",
    "plt.legend([f'Average: {average_percentage:.2f}%'], loc='upper right', fontsize=20, frameon=True, shadow=True, borderpad=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##**Visualisation of Anti-Corruption Measures in the EACC ACT and UNCAC**\n",
    "Analysing and visualising to focus on the prevalence of different anti-corruption measures, specifically those outlined in the EACC ACT and UNCAC frameworks. Combining the data, arranging it, computing the mean values, and presenting the outcomes in a horizontal bar graph. Methodology used entails aggregating data on anti-corruption measures, organising them based on frequency, accounting for the representation of small values, and computing the average value. Subsequently, generating a horizontal bar chart to visually represent the data, incorporating supplementary components such as percentage labels, an average line, and a tailored legend to enhance comprehensibility. The approach is favoured for its ability to utilise data aggregation and sorting to emphasise the most and least common anti-corruption measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Consolidated data for EACC ACT & UNCAC\n",
    "data = {\n",
    "    \"Public Awareness and Education\": 1.6 + 0.5,\n",
    "    \"Whistleblower Protection\": 0.8 + 0.5,\n",
    "    \"Asset Recovery\": 1.1,\n",
    "    \"Code of Conduct for Public Officials\": 0.5,\n",
    "    \"International Cooperation\": 7.1 + 13.9,\n",
    "    \"Strengthening Legal Framework\": 4.1 + 7.9,\n",
    "    \"Monitoring and Evaluation\": 7.1,\n",
    "    \"Institutional Capacity Building\": 2.2,\n",
    "    \"Preventive Policies and Practices\": 12.8 + 14.7,\n",
    "    \"Preventive Anti-Corruption Bodies\": 7.1,\n",
    "    \"Public Sector Transparency\": 6.0 + 4.4,\n",
    "    \"Public Procurement and Management\": 8.2 + 0.5,\n",
    "    \"Public Reporting\": 13.9,\n",
    "    \"Judiciary and Prosecution Services\": 8.2 + 4.1,\n",
    "    \"Private Sector Accountability\": 9.3,\n",
    "    \"Participation of Society\": 1.6,\n",
    "    \"Preventing Money-Laundering\": 3.5\n",
    "}\n",
    "\n",
    "# Sort the data by values\n",
    "sorted_data = {k: v for k, v in sorted(data.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "categories = list(sorted_data.keys())\n",
    "values = list(sorted_data.values())\n",
    "\n",
    "# Increase padding to visualize small values\n",
    "padding = 3.0\n",
    "adjusted_values = [value + padding for value in values]\n",
    "\n",
    "# Calculate average value\n",
    "average_value = np.mean(values)\n",
    "\n",
    "# Create the horizontal bar chart\n",
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "bars = ax.barh(categories, adjusted_values, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Adding percentage labels to the bars\n",
    "def add_labels(bars, values, padding):\n",
    "    for bar, value in zip(bars, values):\n",
    "        width = bar.get_width() - padding\n",
    "        ax.annotate(f'{width:.1f}%', xy=(width, bar.get_y() + bar.get_height() / 2),\n",
    "                    xytext=(3, 0), textcoords=\"offset points\",\n",
    "                    ha='left', va='center', fontsize=14, fontweight='bold', color='black')\n",
    "\n",
    "add_labels(bars, values, padding)\n",
    "\n",
    "# Adding the average line\n",
    "ax.axvline(average_value + padding, color='red', linewidth=2, linestyle='--', label=f'Average: {average_value:.1f}%')\n",
    "\n",
    "# Customizing the plot\n",
    "ax.set_xlabel('Percentage (%)', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Anti-Corruption Measures', fontsize=20, fontweight='bold', color='black')\n",
    "ax.set_title('Prevalence of EACC ACT & UNCAC Anti-Corruption Measures in News Data', fontsize=24, fontweight='bold', color='black')\n",
    "\n",
    "# Make y-axis labels bold and darker\n",
    "ax.set_yticklabels(categories, fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Increase x-axis numbers size and make them darker\n",
    "ax.set_xticklabels(ax.get_xticks(), fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Invert y-axis to have the highest value on top\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Creating the legend box\n",
    "legend_elements = [\n",
    "    Patch(facecolor='skyblue', edgecolor='black', label='EACC ACT & UNCAC Measures'),\n",
    "    Patch(facecolor='none', edgecolor='red', label=f'Average: {average_value:.1f}%')\n",
    "]\n",
    "\n",
    "ax.legend(handles=legend_elements, loc='lower right', fontsize=14, facecolor='white', edgecolor='black', framealpha=1, shadow=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Analysing VSD and VBD Values in Techno-Corruption Text Data**\n",
    "Analysing text content related to techno-corruption to determine the prevalence of specific values and their anti-values. It uses NLP to preprocess text, identify keywords associated with predefined values (VSD and VBD), count their occurrences, and visualise their dominance using bar charts. Data loading and preprocessing uses pandas to load the dataset and remove non-alphanumeric characters and lowercase the text. NLP processing uses SpaCy to tokenize and lemmatize text. Keyword Matching uses predefined VSD and VBD dictionaries to match and count keyword occurrences in text. Percentages and Averages are calculated by calculating each value's percentage dominance and the overall average. Finally, matplotlib is used to visualise the results, with bar charts showing each value's percentage dominance and an average line showing the overall average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load Spacy's English model for NLP tasks\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load data\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "    return text\n",
    "\n",
    "corruptnDF['clean_content'] = corruptnDF['Content'].apply(preprocess_text)\n",
    "\n",
    "# Define VSD and VBD dictionaries\n",
    "vsd_values = {\n",
    "    'Human Welfare': ['wellbeing', 'health', 'quality of life', 'safety', 'support', 'care', 'compassion', 'community'],\n",
    "    'Dignity': ['respect', 'worth', 'self-esteem', 'honour', 'integrity', 'honesty'],\n",
    "    'Justice and Fairness': ['equity', 'equality', 'impartiality', 'fairness', 'justice', 'rights', 'ethical'],\n",
    "    'Autonomy': ['independence', 'freedom', 'self-determination', 'control', 'choice', 'empowerment'],\n",
    "    'Privacy': ['confidentiality', 'secrecy', 'data protection', 'anonymity', 'security'],\n",
    "    'Accountability': ['responsibility', 'transparency', 'answerability', 'liability', 'whistleblowing'],\n",
    "    'Democracy': ['participation', 'inclusion', 'voice', 'voting', 'engagement', 'community'],\n",
    "    'Environmental Sustainability': ['sustainability', 'green', 'eco-friendly', 'conservation', 'renewable'],\n",
    "    'Trust': ['reliability', 'dependability', 'trustworthiness', 'credibility', 'faith']\n",
    "}\n",
    "\n",
    "vbd_values = {\n",
    "    'Effectiveness': ['performance', 'productivity', 'success', 'achievement', 'outcome', 'competition', 'innovation'],\n",
    "    'Efficiency': ['optimisation', 'speed', 'cost-effectiveness', 'resource-saving', 'management', 'digitisation'],\n",
    "    'Satisfaction': ['happiness', 'contentment', 'user experience', 'fulfilment'],\n",
    "    'Safety': ['protection', 'security', 'harm prevention', 'risk management'],\n",
    "    'Reliability': ['dependability', 'consistency', 'stability', 'trustworthiness'],\n",
    "    'Usability': ['user-friendly', 'accessibility', 'ease of use', 'intuitive design'],\n",
    "    'Accessibility': ['inclusivity', 'universal design', 'reachability', 'accommodation'],\n",
    "    'Adaptability': ['flexibility', 'resilience', 'adjustability', 'scalability'],\n",
    "    'Transparency': ['openness', 'clarity', 'visibility', 'honesty', 'exposure'],\n",
    "    'Collaboration': ['teamwork', 'cooperation', 'partnership', 'joint effort']\n",
    "}\n",
    "\n",
    "# Initialize counters\n",
    "vsd_counts = Counter()\n",
    "vbd_counts = Counter()\n",
    "\n",
    "# Function to search and count values using the provided dictionary\n",
    "def search_and_count_values(text, values_dict, counts):\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        for main_value, keywords in values_dict.items():\n",
    "            if lemma in keywords:\n",
    "                counts[main_value] += 1\n",
    "\n",
    "# Apply function (vectorized approach to improve performance)\n",
    "corruptnDF['clean_content'].dropna().apply(lambda text: search_and_count_values(text, vsd_values, vsd_counts))\n",
    "corruptnDF['clean_content'].dropna().apply(lambda text: search_and_count_values(text, vbd_values, vbd_counts))\n",
    "\n",
    "# Calculate percentages and averages\n",
    "def calculate_percentages_and_average(counts):\n",
    "    total_mentions = sum(counts.values())\n",
    "    percentages = {value: (count / total_mentions) * 100 for value, count in counts.items()}\n",
    "    average = sum(percentages.values()) / len(percentages)\n",
    "    return percentages, average\n",
    "\n",
    "vsd_percentages, vsd_average = calculate_percentages_and_average(vsd_counts)\n",
    "vbd_percentages, vbd_average = calculate_percentages_and_average(vbd_counts)\n",
    "\n",
    "# Visualization\n",
    "def plot_values(values_percentages, average, title, color):\n",
    "    labels = list(values_percentages.keys())\n",
    "    percentages = list(values_percentages.values())\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    bars = plt.bar(labels, percentages, color=color)\n",
    "    plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}%')\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2.0, height + 0.5, f'{height:.2f}%', ha='center', va='bottom')\n",
    "\n",
    "    # Adding padding to the bars\n",
    "    for bar in bars:\n",
    "        if bar.get_height() < 5:\n",
    "            bar.set_height(5)\n",
    "\n",
    "    plt.xlabel('Values', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Percentage of Dominance (%)', fontsize=14, fontweight='bold')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=12, fontweight='bold')\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_values(vsd_percentages, vsd_average, 'VSD Values Dominance', 'steelblue')\n",
    "plot_values(vbd_percentages, vbd_average, 'VBD Values Dominance', 'mediumseagreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Analysing VSD and VBD Anti-Values in Techno-Corruption Text Data**\n",
    "Analysing techno-corruption text data for anti-values (negative VSD and VBD values). Identifying keywords associated with predefined anti-values, counting their occurrences, and displaying their dominance in bar charts. This identification of dominant anti-values in techno-corruption involves highlighting the most negative aspects. The percentage is calculated in quantitative analysis to determine the dominance of each anti-value. Finally, visual representation is done using bar charts to compare the dominance of different anti-values, identifying critical negative values that require policy and legislative action.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load Spacy's English model for NLP tasks\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Load data\n",
    "excel_file_path = '318NewsDataSet.xlsx'\n",
    "corruptnDF = pd.read_excel(excel_file_path, header=None, names=['Topic', 'Content', 'Concepts', 'Variables', 'KeyWords', 'Context', 'Month', 'Year'])\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower())\n",
    "    return text\n",
    "\n",
    "corruptnDF['clean_content'] = corruptnDF['Content'].apply(preprocess_text)\n",
    "\n",
    "# Define VSD and VBD antivalues dictionaries\n",
    "vsd_antivalues = {\n",
    "    'Suffering': ['illness', 'poor quality of life', 'danger', 'neglect', 'harm', 'indifference', 'alienation'],\n",
    "    'Disrespect': ['worthlessness', 'self-degradation', 'dishonour', 'corruption', 'dishonesty'],\n",
    "    'Injustice': ['inequity', 'inequality', 'bias', 'unfairness', 'rights violations', 'unethical behaviour'],\n",
    "    'Dependence': ['restriction', 'subjugation', 'lack of control', 'coercion', 'disempowerment'],\n",
    "    'Exposure': ['data breach', 'surveillance', 'insecurity'],\n",
    "    'Irresponsibility': ['opacity', 'unanswerability', 'impunity', 'non-transparency', 'lack of whistleblowing'],\n",
    "    'Exclusion': ['suppression', 'silence', 'autocracy', 'disengagement', 'non-participation', 'community breakdown'],\n",
    "    'Unsustainability': ['pollution', 'eco-unfriendly practices', 'depletion', 'nonrenewable usage'],\n",
    "    'Mistrust': ['unreliability', 'untrustworthiness', 'deception', 'skepticism']\n",
    "}\n",
    "\n",
    "vbd_antivalues = {\n",
    "    'Ineffectiveness': ['inefficiency', 'failure', 'incompetence', 'poor outcomes', 'non-competitive', 'lack of innovation'],\n",
    "    'Inefficiency': ['slowness', 'wastefulness', 'resource drain', 'mismanagement', 'lack of digitization'],\n",
    "    'Dissatisfaction': ['unhappiness', 'discontent', 'poor user experience'],\n",
    "    'Danger': ['insecurity', 'harm', 'risk', 'vulnerability'],\n",
    "    'Unreliability': ['inconsistency', 'instability', 'untrustworthiness'],\n",
    "    'Complexity': ['inaccessibility', 'difficulty of use', 'non-intuitive design'],\n",
    "    'Exclusivity': ['inaccessibility', 'barriers', 'lack of accommodation'],\n",
    "    'Rigidity': ['inflexibility', 'fragility', 'inability to adjust'],\n",
    "    'Secrecy': ['obfuscation', 'invisibility', 'dishonesty', 'lack of exposure'],\n",
    "    'Isolation': ['conflict', 'disunity', 'lack of cooperation']\n",
    "}\n",
    "\n",
    "# Initialize counters\n",
    "vsd_antivalues_counts = Counter()\n",
    "vbd_antivalues_counts = Counter()\n",
    "\n",
    "# Function to search and count values using the provided dictionary\n",
    "def search_and_count_values(text, values_dict, counts):\n",
    "    doc = nlp(text.lower())\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        for main_value, keywords in values_dict.items():\n",
    "            if lemma in keywords:\n",
    "                counts[main_value] += 1\n",
    "\n",
    "# Apply function (vectorized approach to improve performance)\n",
    "corruptnDF['clean_content'].dropna().apply(lambda text: search_and_count_values(text, vsd_antivalues, vsd_antivalues_counts))\n",
    "corruptnDF['clean_content'].dropna().apply(lambda text: search_and_count_values(text, vbd_antivalues, vbd_antivalues_counts))\n",
    "\n",
    "# Calculate percentages and averages\n",
    "def calculate_percentages_and_average(counts):\n",
    "    total_mentions = sum(counts.values())\n",
    "    percentages = {value: (count / total_mentions) * 100 for value, count in counts.items()}\n",
    "    average = sum(percentages.values()) / len(percentages)\n",
    "    return percentages, average\n",
    "\n",
    "vsd_antivalues_percentages, vsd_antivalues_average = calculate_percentages_and_average(vsd_antivalues_counts)\n",
    "vbd_antivalues_percentages, vbd_antivalues_average = calculate_percentages_and_average(vbd_antivalues_counts)\n",
    "\n",
    "# Visualization\n",
    "def plot_values(values_percentages, average, title, color):\n",
    "    labels = list(values_percentages.keys())\n",
    "    percentages = list(values_percentages.values())\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    bars = plt.bar(labels, percentages, color=color)\n",
    "    plt.axhline(y=average, color='r', linestyle='--', label=f'Average: {average:.2f}%')\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        adjusted_height = max(height, 5)  # Ensure minimum bar height for visibility\n",
    "        plt.text(bar.get_x() + bar.get_width() / 2.0, adjusted_height + 0.5, f'{height:.2f}%', ha='center', va='bottom')\n",
    "        if height < 5:\n",
    "            bar.set_height(5)  # Set the minimum height for the bar\n",
    "\n",
    "    plt.xlabel('Anti Values', fontsize=14, fontweight='bold')\n",
    "    plt.ylabel('Percentage of Dominance (%)', fontsize=14, fontweight='bold')\n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=12, fontweight='bold')\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_values(vsd_antivalues_percentages, vsd_antivalues_average, 'VSD Anti-Values Dominance', 'purple')\n",
    "plot_values(vbd_antivalues_percentages, vbd_antivalues_average, 'VBD Anti-Values Dominance', 'firebrick')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Comparative Analysis of VSD Values and Anti-Values in Techno-Corruption DataSet**\n",
    "Compares techno-corruption text's VSD (Value-Sensitive Design) and anti-value prevalence in a news dataset. It processes the text, finds keywords associated with predefined values and anti-values, calculates their percentages, and displays the results in a horizontal bar chart. Uses Value-Sensitive Design (VSD), which considers human values throughout the design process. Keyword Matching and Counting quantifies values and anti-values by matching text tokens to predefined lists. This insightful comparison compares positive and negative anti-values in techno-corruption. Further, Quantitative Analysis quantifies the prevalence of each value and anti-value. Finally, providing visual clarity by highlighting dominant values and anti-values using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Percentage values from the provided graphs\n",
    "values_percentages = {\n",
    "    'Privacy': 16.92,\n",
    "    'Human Welfare': 44.58,\n",
    "    'Justice and Fairness': 6.79,\n",
    "    'Autonomy': 7.89,\n",
    "    'Accountability': 6.46,\n",
    "    'Dignity': 5.91,\n",
    "    'Democracy': 9.18,\n",
    "    'Environmental Sustainability': 1.71,\n",
    "    'Trust': 0.55\n",
    "}\n",
    "\n",
    "antivalues_percentages = {\n",
    "    'Disrespect': 81.93,\n",
    "    'Dependence': 0.80,\n",
    "    'Mistrust': 0.16,\n",
    "    'Exposure': 3.50,\n",
    "    'Irresponsibility': 3.66,\n",
    "    'Suffering': 3.34,\n",
    "    'Injustice': 5.57,\n",
    "    'Unsustainability': 0.24,\n",
    "    'Exclusion': 0.80\n",
    "}\n",
    "\n",
    "# Matching antivalues categories\n",
    "matching_antivalues = {\n",
    "    'Human Welfare': 'Suffering',\n",
    "    'Dignity': 'Disrespect',\n",
    "    'Justice and Fairness': 'Injustice',\n",
    "    'Autonomy': 'Dependence',\n",
    "    'Privacy': 'Exposure',\n",
    "    'Accountability': 'Irresponsibility',\n",
    "    'Democracy': 'Exclusion',\n",
    "    'Environmental Sustainability': 'Unsustainability',\n",
    "    'Trust': 'Mistrust'\n",
    "}\n",
    "\n",
    "# Combine values and antivalues for sorting\n",
    "combined_percentages = {key: (values_percentages[key], antivalues_percentages[matching_antivalues[key]]) for key in values_percentages}\n",
    "\n",
    "# Sort categories by anti-values percentage in ascending order\n",
    "sorted_categories = sorted(combined_percentages.keys(), key=lambda k: combined_percentages[k][1])\n",
    "\n",
    "values_list = [values_percentages[cat] for cat in sorted_categories]\n",
    "antivalues_list = [antivalues_percentages[matching_antivalues[cat]] for cat in sorted_categories]\n",
    "\n",
    "# Calculate averages\n",
    "average_values = np.mean(values_list)\n",
    "average_antivalues = np.mean(antivalues_list)\n",
    "\n",
    "x = np.arange(len(sorted_categories))  # label locations\n",
    "width = 0.3  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Horizontal bar plot\n",
    "rects1 = ax.barh(x - width / 2, values_list, width, label='VSD Values', color='steelblue')\n",
    "rects2 = ax.barh(x + width / 2, antivalues_list, width, label='VSD Anti-Values', color='purple')\n",
    "\n",
    "# Add faint average lines\n",
    "ax.axvline(average_values, color='steelblue', linestyle='--', linewidth=2, alpha=0.5, label=f'Average Values: {average_values:.2f}%')\n",
    "ax.axvline(average_antivalues, color='purple', linestyle='--', linewidth=2, alpha=0.5, label=f'Average Anti-Values: {average_antivalues:.2f}%')\n",
    "\n",
    "# Add labels, title, and custom tick labels\n",
    "ax.set_xlabel('Percentage of Dominance (%)', fontsize=18, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Categories', fontsize=18, fontweight='bold', color='black')\n",
    "ax.set_title('VSD Values and Anti-Values Comparison', fontsize=24, fontweight='bold', color='black')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(sorted_categories, fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Move the legend to the bottom corner\n",
    "ax.legend(fontsize=14, loc='lower right')\n",
    "\n",
    "# Add percentage labels and visual cue for small values\n",
    "def add_labels(rects, labels, actual_values, color):\n",
    "    for rect, label, actual_value in zip(rects, labels, actual_values):\n",
    "        width = rect.get_width()\n",
    "        text = f'{label} ({actual_value}%)'\n",
    "        ax.annotate(text,\n",
    "                    xy=(width, rect.get_y() + rect.get_height() / 2),\n",
    "                    xytext=(10, 0),  # 10 points horizontal offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center', fontsize=14)\n",
    "        if actual_value < 1.0:  # Add marker for very small values\n",
    "            ax.annotate('•', xy=(0, rect.get_y() + rect.get_height() / 2), color=color,\n",
    "                        ha='right', va='center', fontsize=14)\n",
    "\n",
    "# Actual values for labels\n",
    "actual_values_list = [values_percentages[cat] for cat in sorted_categories]\n",
    "actual_antivalues_list = [antivalues_percentages[matching_antivalues[cat]] for cat in sorted_categories]\n",
    "\n",
    "add_labels(rects1, sorted_categories, actual_values_list, 'steelblue')\n",
    "add_labels(rects2, [matching_antivalues[cat] for cat in sorted_categories], actual_antivalues_list, 'purple')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tabular Visualisation of VSD Values and Anti-Values**\n",
    "Creating a clear table of VSD values and their anti-values. Then colouring the table for enhanced content readability. Uses data preparation method to define values, anti-values, and descriptions. Matplotlib is used to create a table with colour-coded cells for values and anti-values distinction. Tabular Visualisation is preferred for its structured data presentation for easy reference and comparison. Colour coding improves visual distinction between table data categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create tables with background color\n",
    "table_data = [\n",
    "    ['Values & Anti-Values', 'Types'],\n",
    "    ['Human Welfare', 'Wellbeing, health, quality of life, safety, support, care, compassion, community'],\n",
    "    ['Dignity', 'Respect, worth, self-esteem, honour, integrity, honesty'],\n",
    "    ['Justice and Fairness', 'Equity, equality, impartiality, fairness, justice, rights, ethical'],\n",
    "    ['Autonomy', 'Independence, freedom, self-determination, control, choice, empowerment'],\n",
    "    ['Privacy', 'Confidentiality, secrecy, data protection, anonymity, security'],\n",
    "    ['Accountability', 'Responsibility, transparency, answerability, liability, whistleblowing'],\n",
    "    ['Democracy', 'Participation, inclusion, voice, voting, engagement, community'],\n",
    "    ['Environmental Sustainability', 'Sustainability, green, eco-friendly, conservation, renewable'],\n",
    "    ['Trust', 'Reliability, dependability, trustworthiness, credibility, faith'],\n",
    "    ['Suffering', 'Illness, poor quality of life, danger, neglect, harm, indifference, alienation'],\n",
    "    ['Disrespect', 'Worthlessness, self-degradation, dishonour, corruption, dishonesty'],\n",
    "    ['Injustice', 'Inequity, inequality, bias, unfairness, rights violations, unethical behaviour'],\n",
    "    ['Dependence', 'Restriction, subjugation, lack of control, coercion, disempowerment'],\n",
    "    ['Exposure', 'Data breach, surveillance, insecurity'],\n",
    "    ['Irresponsibility', 'Opacity, unanswerability, impunity, non-transparency, lack of whistleblowing'],\n",
    "    ['Exclusion', 'Suppression, silence, autocracy, disengagement, non-participation, community breakdown'],\n",
    "    ['Unsustainability', 'Pollution, eco-unfriendly practices, depletion, nonrenewable usage'],\n",
    "    ['Mistrust', 'Unreliability, untrustworthiness, deception, skepticism']\n",
    "]\n",
    "\n",
    "header_colors = [['#d0e1f2']*2]\n",
    "values_colors = [['#f7f7f7']*2]*9  # Light grey for values\n",
    "antivalues_colors = [['#e1d0f2']*2]*9  # Light purple for anti-values\n",
    "cell_colours = header_colors + values_colors + antivalues_colors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                 cellColours=cell_colours,\n",
    "                 loc='center',\n",
    "                 cellLoc='left',\n",
    "                 colWidths=[0.3, 0.7],\n",
    "                 bbox=[0.0, 0.0, 1.0, 1.0])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.auto_set_column_width(col=list(range(len(table_data[0]))))\n",
    "\n",
    "# Adjust row height for compactness\n",
    "for key, cell in table.get_celld().items():\n",
    "    cell.set_height(0.04)\n",
    "    cell.set_text_props(color='black')  # Set text color to dark black\n",
    "\n",
    "# Bold and center subtitles\n",
    "table[0, 0].set_text_props(fontweight='bold', ha='center', color='black')\n",
    "table[0, 1].set_text_props(fontweight='bold', ha='center', color='black')\n",
    "\n",
    "# Left align all rows except the header\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(len(table_data[i])):\n",
    "        table[i, j].set_text_props(ha='left', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#**Comparative Analysis of VBD Values and Anti-Values in Techno-Corruption Dataset**\n",
    "Analysing the prevalence of VBD (Value-Based Design) values and anti-values in techno-corruption. Processing the data, finding keywords associated with predefined values and anti-values, calculating their percentages, and displaying the results in a horizontal bar chart. Data Preparation loads Excel data and removes non-alphanumeric characters and converts to lowercase. Uses the Spacy NLP library to match keywords to predefined VBD values and anti-values, counting their occurrences, and calculating their percentage dominance. Finally, visualising output by creating a horizontal bar chart with values and anti-values sorted in ascending order by anti-value percentages and an average line showing overall dominance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Percentage values for VBD Values\n",
    "vbd_values_percentages = {\n",
    "    'Safety': 54.87,\n",
    "    'Efficiency': 16.41,\n",
    "    'Effectiveness': 14.99,\n",
    "    'Reliability': 1.32,\n",
    "    'Collaboration': 8.10,\n",
    "    'Transparency': 1.05,\n",
    "    'Usability': 0.58,\n",
    "    'Accessibility': 0.32,\n",
    "    'Adaptability': 2.21,\n",
    "    'Satisfaction': 0.16\n",
    "}\n",
    "\n",
    "# Percentage values for VBD Anti-Values\n",
    "vbd_antivalues_percentages = {\n",
    "    'Isolation': 14.41,\n",
    "    'Danger': 52.70,\n",
    "    'Ineffectiveness': 21.85,\n",
    "    'Unreliability': 2.93,\n",
    "    'Inefficiency': 5.41,\n",
    "    'Rigidity': 1.80,\n",
    "    'Dissatisfaction': 0.68,\n",
    "    'Secrecy': 0.23,\n",
    "    'Complexity': 0.0,  # Assume 0% if not provided\n",
    "    'Exclusivity': 0.0  # Assume 0% if not provided\n",
    "}\n",
    "\n",
    "# Matching VBD Anti-Values categories\n",
    "matching_vbd_antivalues = {\n",
    "    'Safety': 'Danger',\n",
    "    'Efficiency': 'Inefficiency',\n",
    "    'Effectiveness': 'Ineffectiveness',\n",
    "    'Reliability': 'Unreliability',\n",
    "    'Collaboration': 'Isolation',\n",
    "    'Transparency': 'Secrecy',\n",
    "    'Usability': 'Complexity',\n",
    "    'Accessibility': 'Exclusivity',\n",
    "    'Adaptability': 'Rigidity',\n",
    "    'Satisfaction': 'Dissatisfaction'\n",
    "}\n",
    "\n",
    "# Combine values and antivalues for sorting\n",
    "combined_vbd_percentages = {key: (vbd_values_percentages[key], vbd_antivalues_percentages[matching_vbd_antivalues[key]]) for key in vbd_values_percentages}\n",
    "\n",
    "# Sort categories by anti-values percentage in ascending order\n",
    "sorted_vbd_categories = sorted(combined_vbd_percentages.keys(), key=lambda k: combined_vbd_percentages[k][1])\n",
    "\n",
    "vbd_values_list = [vbd_values_percentages[cat] for cat in sorted_vbd_categories]\n",
    "vbd_antivalues_list = [vbd_antivalues_percentages[matching_vbd_antivalues[cat]] for cat in sorted_vbd_categories]\n",
    "\n",
    "# Calculate averages\n",
    "average_vbd_values = np.mean(vbd_values_list)\n",
    "average_vbd_antivalues = np.mean(vbd_antivalues_list)\n",
    "\n",
    "x = np.arange(len(sorted_vbd_categories))  # label locations\n",
    "width = 0.3  # bar width\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Horizontal bar plot with original colors\n",
    "rects1 = ax.barh(x - width / 2, vbd_values_list, width, label='VBD Values', color='green')\n",
    "rects2 = ax.barh(x + width / 2, vbd_antivalues_list, width, label='VBD Anti-Values', color='red')\n",
    "\n",
    "# Add faint average lines\n",
    "ax.axvline(average_vbd_values, color='green', linestyle='--', linewidth=2, alpha=0.5, label=f'Average Values: {average_vbd_values:.2f}%')\n",
    "ax.axvline(average_vbd_antivalues, color='red', linestyle='--', linewidth=2, alpha=0.5, label=f'Average Anti-Values: {average_vbd_antivalues:.2f}%')\n",
    "\n",
    "# Add labels, title, and custom tick labels\n",
    "ax.set_xlabel('Percentage of Dominance (%)', fontsize=18, fontweight='bold', color='black')\n",
    "ax.set_ylabel('Categories', fontsize=18, fontweight='bold', color='black')\n",
    "ax.set_title('VBD Values and Anti-Values Comparison', fontsize=24, fontweight='bold', color='black')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(sorted_vbd_categories, fontsize=16, fontweight='bold', color='black')\n",
    "\n",
    "# Move the legend to the bottom corner\n",
    "ax.legend(fontsize=14, loc='lower right')\n",
    "\n",
    "# Add percentage labels and visual cue for small values\n",
    "def add_labels(rects, labels, actual_values, color):\n",
    "    for rect, label, actual_value in zip(rects, labels, actual_values):\n",
    "        width = rect.get_width()\n",
    "        text = f'{label} ({actual_value}%)'\n",
    "        ax.annotate(text,\n",
    "                    xy=(width, rect.get_y() + rect.get_height() / 2),\n",
    "                    xytext=(10, 0),  # 10 points horizontal offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='left', va='center', fontsize=14)\n",
    "        if actual_value < 1.0:  # Add marker for very small values\n",
    "            ax.annotate('•', xy=(0, rect.get_y() + rect.get_height() / 2), color=color,\n",
    "                        ha='right', va='center', fontsize=14)\n",
    "\n",
    "# Actual values for labels\n",
    "actual_vbd_values_list = [vbd_values_percentages[cat] for cat in sorted_vbd_categories]\n",
    "actual_vbd_antivalues_list = [vbd_antivalues_percentages[matching_vbd_antivalues[cat]] for cat in sorted_vbd_categories]\n",
    "\n",
    "add_labels(rects1, sorted_vbd_categories, actual_vbd_values_list, 'green')\n",
    "add_labels(rects2, [matching_vbd_antivalues[cat] for cat in sorted_vbd_categories], actual_vbd_antivalues_list, 'red')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tabular Visualisation of VBD Values and Anti-Values**\n",
    "Generates a visually concise table that presents the values of Value-Based Design (VBD) and their corresponding anti-values. The methodology applies colour coding to the table to improve readability and adjust the layout to make it more compact. Uses data preparation methodologies to establish the values and anti-values, along with their corresponding descriptions. Followed with table creation using matplotlib to generate a table that includes cells with distinct colours to differentiate between positive and negative values. Tabular Visualisation is favoured due to its ability to present data in a systematic format, facilitating convenient referencing and comparison. Colour coding is employed to augment the visual differentiation among different data categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create tables with background color\n",
    "table_data = [\n",
    "    ['Values & Anti-Values', 'Types'],\n",
    "    ['Effectiveness', 'Performance, productivity, success, achievement, outcome, competition, innovation'],\n",
    "    ['Efficiency', 'Optimisation, speed, cost-effectiveness, resource-saving, management, digitisation'],\n",
    "    ['Satisfaction', 'Happiness, contentment, user experience, fulfilment'],\n",
    "    ['Safety', 'Protection, security, harm prevention, risk management'],\n",
    "    ['Reliability', 'Dependability, consistency, stability, trustworthiness'],\n",
    "    ['Usability', 'User-friendly, accessibility, ease of use, intuitive design'],\n",
    "    ['Accessibility', 'Inclusivity, universal design, reachability, accommodation'],\n",
    "    ['Adaptability', 'Flexibility, resilience, adjustability, scalability'],\n",
    "    ['Transparency', 'Openness, clarity, visibility, honesty, exposure'],\n",
    "    ['Collaboration', 'Teamwork, cooperation, partnership, joint effort'],\n",
    "    ['Ineffectiveness', 'Inefficiency, failure, incompetence, poor outcomes, non-competitive, lack of innovation'],\n",
    "    ['Inefficiency', 'Slowness, wastefulness, resource drain, mismanagement, lack of digitization'],\n",
    "    ['Dissatisfaction', 'Unhappiness, discontent, poor user experience'],\n",
    "    ['Danger', 'Insecurity, harm, risk, vulnerability'],\n",
    "    ['Unreliability', 'Inconsistency, instability, untrustworthiness'],\n",
    "    ['Complexity', 'Inaccessibility, difficulty of use, non-intuitive design'],\n",
    "    ['Exclusivity', 'Inaccessibility, barriers, lack of accommodation'],\n",
    "    ['Rigidity', 'Inflexibility, fragility, inability to adjust'],\n",
    "    ['Secrecy', 'Obfuscation, invisibility, dishonesty, lack of exposure'],\n",
    "    ['Isolation', 'Conflict, disunity, lack of cooperation']\n",
    "]\n",
    "\n",
    "header_colors = [['#d0e1f2']*2]\n",
    "values_colors = [['#d4edda']*2]*10  # Light green for values\n",
    "antivalues_colors = [['#f8d7da']*2]*10  # Light red for anti-values\n",
    "cell_colours = header_colors + values_colors + antivalues_colors\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "table = ax.table(cellText=table_data,\n",
    "                 cellColours=cell_colours,\n",
    "                 loc='center',\n",
    "                 cellLoc='left',\n",
    "                 colWidths=[0.3, 0.7],\n",
    "                 bbox=[0.0, 0.0, 1.0, 1.0])\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(14)\n",
    "table.auto_set_column_width(col=list(range(len(table_data[0]))))\n",
    "\n",
    "# Adjust row height for compactness\n",
    "for key, cell in table.get_celld().items():\n",
    "    cell.set_height(0.04)\n",
    "    cell.set_text_props(color='black')  # Set text color to dark black\n",
    "\n",
    "# Bold and center subtitles\n",
    "table[0, 0].set_text_props(fontweight='bold', ha='center', color='black')\n",
    "table[0, 1].set_text_props(fontweight='bold', ha='center', color='black')\n",
    "\n",
    "# Left align all rows except the header\n",
    "for i in range(1, len(table_data)):\n",
    "    for j in range(len(table_data[i])):\n",
    "        table[i, j].set_text_props(ha='left', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
